[2024-06-12T10:41:03.200+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T10:41:03.218+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T10:41:03.224+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T10:41:03.224+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T10:41:03.233+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T10:41:03.238+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=950) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T10:41:03.239+0000] {standard_task_runner.py:63} INFO - Started process 957 to run task
[2024-06-12T10:41:03.240+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '34', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmp53div8h5']
[2024-06-12T10:41:03.241+0000] {standard_task_runner.py:91} INFO - Job 34: Subtask fetch_and_publish_driver
[2024-06-12T10:41:03.274+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host 52285cb34a44
[2024-06-12T10:41:03.326+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T10:41:03.327+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T10:41:05.004+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T10:41:05.006+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.18.0.6', 9092)]>: connecting to kafka:9092 [('172.18.0.6', 9092) IPv4]
[2024-06-12T10:41:05.006+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T10:41:05.007+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.18.0.6', 9092)]>: Connection complete.
[2024-06-12T10:41:05.110+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T10:41:05.110+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T10:41:05.265+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.18.0.6', 9092)]>: connecting to kafka:9092 [('172.18.0.6', 9092) IPv4]
[2024-06-12T10:41:05.265+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.18.0.6', 9092)]>: Connection complete.
[2024-06-12T10:41:05.266+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.18.0.6', 9092)]>: Closing connection. 
[2024-06-12T10:41:05.379+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T10:41:05.425+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.18.0.6', 9092)]>: Closing connection. 
[2024-06-12T10:41:05.426+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T10:41:05.426+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T10:41:05.434+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T104103, end_date=20240612T104105
[2024-06-12T10:41:05.460+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T10:41:05.473+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-12T10:41:05.474+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T11:06:13.842+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T11:06:13.857+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T11:06:13.863+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T11:06:13.863+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T11:06:13.875+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T11:06:13.879+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=676) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T11:06:13.880+0000] {standard_task_runner.py:63} INFO - Started process 694 to run task
[2024-06-12T11:06:13.881+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '34', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmp4q6fyagq']
[2024-06-12T11:06:13.883+0000] {standard_task_runner.py:91} INFO - Job 34: Subtask fetch_and_publish_driver
[2024-06-12T11:06:13.921+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host a8d8326d12bd
[2024-06-12T11:06:13.988+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T11:06:13.989+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T11:06:15.871+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T11:06:15.872+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.19.0.6', 9092)]>: connecting to kafka:9092 [('172.19.0.6', 9092) IPv4]
[2024-06-12T11:06:15.873+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T11:06:15.873+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.19.0.6', 9092)]>: Connection complete.
[2024-06-12T11:06:15.976+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T11:06:15.976+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T11:06:16.131+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.19.0.6', 9092)]>: connecting to kafka:9092 [('172.19.0.6', 9092) IPv4]
[2024-06-12T11:06:16.132+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.19.0.6', 9092)]>: Connection complete.
[2024-06-12T11:06:16.132+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.19.0.6', 9092)]>: Closing connection. 
[2024-06-12T11:06:16.352+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T11:06:16.358+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.19.0.6', 9092)]>: Closing connection. 
[2024-06-12T11:06:16.359+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T11:06:16.360+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T11:06:16.368+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T110613, end_date=20240612T110616
[2024-06-12T11:06:16.422+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T11:06:16.429+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T11:18:35.725+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T11:18:35.739+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T11:18:35.744+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T11:18:35.745+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T11:18:35.755+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T11:18:35.760+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=578) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T11:18:35.761+0000] {standard_task_runner.py:63} INFO - Started process 591 to run task
[2024-06-12T11:18:35.762+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '33', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmpik4iarft']
[2024-06-12T11:18:35.763+0000] {standard_task_runner.py:91} INFO - Job 33: Subtask fetch_and_publish_driver
[2024-06-12T11:18:35.797+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host e07cec2adf81
[2024-06-12T11:18:35.855+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T11:18:35.856+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T11:18:38.310+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T11:18:38.312+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.20.0.6', 9092)]>: connecting to kafka:9092 [('172.20.0.6', 9092) IPv4]
[2024-06-12T11:18:38.312+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T11:18:38.313+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.20.0.6', 9092)]>: Connection complete.
[2024-06-12T11:18:38.415+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T11:18:38.415+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T11:18:38.571+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.20.0.6', 9092)]>: connecting to kafka:9092 [('172.20.0.6', 9092) IPv4]
[2024-06-12T11:18:38.571+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.20.0.6', 9092)]>: Connection complete.
[2024-06-12T11:18:38.571+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.20.0.6', 9092)]>: Closing connection. 
[2024-06-12T11:18:38.684+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T11:18:38.687+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.20.0.6', 9092)]>: Closing connection. 
[2024-06-12T11:18:38.688+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T11:18:38.688+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T11:18:38.696+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T111835, end_date=20240612T111838
[2024-06-12T11:18:38.745+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T11:18:38.760+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-12T11:18:38.762+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T11:24:45.582+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T11:24:45.598+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T11:24:45.605+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T11:24:45.605+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T11:24:45.615+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T11:24:45.621+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=68) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T11:24:45.624+0000] {standard_task_runner.py:63} INFO - Started process 77 to run task
[2024-06-12T11:24:45.623+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmpso5vyjdj']
[2024-06-12T11:24:45.627+0000] {standard_task_runner.py:91} INFO - Job 8: Subtask fetch_and_publish_driver
[2024-06-12T11:24:45.677+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host 16e46b42a87a
[2024-06-12T11:24:45.753+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T11:24:45.754+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T11:24:48.879+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T11:24:48.881+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.21.0.6', 9092)]>: connecting to kafka:9092 [('172.21.0.6', 9092) IPv4]
[2024-06-12T11:24:48.881+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T11:24:48.882+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.21.0.6', 9092)]>: Connection complete.
[2024-06-12T11:24:48.985+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T11:24:48.986+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T11:24:49.141+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.21.0.6', 9092)]>: connecting to kafka:9092 [('172.21.0.6', 9092) IPv4]
[2024-06-12T11:24:49.142+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.21.0.6', 9092)]>: Connection complete.
[2024-06-12T11:24:49.142+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.21.0.6', 9092)]>: Closing connection. 
[2024-06-12T11:24:49.261+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T11:24:49.267+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.21.0.6', 9092)]>: Closing connection. 
[2024-06-12T11:24:49.268+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T11:24:49.269+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T11:24:49.276+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T112445, end_date=20240612T112449
[2024-06-12T11:24:49.292+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T11:24:49.298+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T11:41:08.359+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T11:41:08.379+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T11:41:08.388+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T11:41:08.388+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T11:41:08.398+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T11:41:08.403+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=620) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T11:41:08.406+0000] {standard_task_runner.py:63} INFO - Started process 629 to run task
[2024-06-12T11:41:08.406+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '36', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmpeylqmg5m']
[2024-06-12T11:41:08.408+0000] {standard_task_runner.py:91} INFO - Job 36: Subtask fetch_and_publish_driver
[2024-06-12T11:41:08.459+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host 33197dc676ae
[2024-06-12T11:41:08.530+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T11:41:08.531+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T11:41:09.782+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T11:41:09.784+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.22.0.6', 9092)]>: connecting to kafka:9092 [('172.22.0.6', 9092) IPv4]
[2024-06-12T11:41:09.784+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T11:41:09.785+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.22.0.6', 9092)]>: Connection complete.
[2024-06-12T11:41:09.888+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T11:41:09.889+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T11:41:10.043+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.22.0.6', 9092)]>: connecting to kafka:9092 [('172.22.0.6', 9092) IPv4]
[2024-06-12T11:41:10.043+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.22.0.6', 9092)]>: Connection complete.
[2024-06-12T11:41:10.043+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.22.0.6', 9092)]>: Closing connection. 
[2024-06-12T11:41:10.158+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T11:41:10.162+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.22.0.6', 9092)]>: Closing connection. 
[2024-06-12T11:41:10.163+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T11:41:10.163+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T11:41:10.171+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T114108, end_date=20240612T114110
[2024-06-12T11:41:10.186+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T11:41:10.199+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-12T11:41:10.201+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T11:56:39.479+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T11:56:39.499+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T11:56:39.505+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T11:56:39.505+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T11:56:39.514+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T11:56:39.521+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=585) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T11:56:39.522+0000] {standard_task_runner.py:63} INFO - Started process 598 to run task
[2024-06-12T11:56:39.522+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '35', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmpco_npy4y']
[2024-06-12T11:56:39.524+0000] {standard_task_runner.py:91} INFO - Job 35: Subtask fetch_and_publish_driver
[2024-06-12T11:56:39.563+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host e699387b7fc9
[2024-06-12T11:56:39.624+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T11:56:39.625+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T11:56:41.702+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T11:56:41.704+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.23.0.6', 9092)]>: connecting to kafka:9092 [('172.23.0.6', 9092) IPv4]
[2024-06-12T11:56:41.704+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T11:56:41.705+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.23.0.6', 9092)]>: Connection complete.
[2024-06-12T11:56:41.807+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T11:56:41.808+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T11:56:41.962+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.23.0.6', 9092)]>: connecting to kafka:9092 [('172.23.0.6', 9092) IPv4]
[2024-06-12T11:56:41.963+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.23.0.6', 9092)]>: Connection complete.
[2024-06-12T11:56:41.963+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.23.0.6', 9092)]>: Closing connection. 
[2024-06-12T11:56:42.078+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T11:56:42.082+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.23.0.6', 9092)]>: Closing connection. 
[2024-06-12T11:56:42.083+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T11:56:42.083+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T11:56:42.090+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T115639, end_date=20240612T115642
[2024-06-12T11:56:42.144+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T11:56:42.158+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-12T11:56:42.160+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T12:09:07.666+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T12:09:07.684+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T12:09:07.690+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T12:09:07.690+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T12:09:07.698+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T12:09:07.702+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=642) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T12:09:07.703+0000] {standard_task_runner.py:63} INFO - Started process 650 to run task
[2024-06-12T12:09:07.703+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmp4xhwtroj']
[2024-06-12T12:09:07.705+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask fetch_and_publish_driver
[2024-06-12T12:09:07.739+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host 7c9178d6b02a
[2024-06-12T12:09:07.797+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T12:09:07.798+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T12:09:11.434+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T12:09:11.435+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.24.0.6', 9092)]>: connecting to kafka:9092 [('172.24.0.6', 9092) IPv4]
[2024-06-12T12:09:11.436+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T12:09:11.436+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.24.0.6', 9092)]>: Connection complete.
[2024-06-12T12:09:11.539+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T12:09:11.540+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T12:09:11.694+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.24.0.6', 9092)]>: connecting to kafka:9092 [('172.24.0.6', 9092) IPv4]
[2024-06-12T12:09:11.694+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.24.0.6', 9092)]>: Connection complete.
[2024-06-12T12:09:11.695+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.24.0.6', 9092)]>: Closing connection. 
[2024-06-12T12:09:11.807+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T12:09:11.811+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.24.0.6', 9092)]>: Closing connection. 
[2024-06-12T12:09:11.811+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T12:09:11.812+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T12:09:11.818+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T120907, end_date=20240612T120911
[2024-06-12T12:09:11.851+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T12:09:11.865+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-12T12:09:11.866+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T12:33:39.774+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T12:33:39.792+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T12:33:39.799+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T12:33:39.799+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T12:33:39.807+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T12:33:39.812+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=649) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T12:33:39.813+0000] {standard_task_runner.py:63} INFO - Started process 656 to run task
[2024-06-12T12:33:39.813+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '34', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmpck3oukbx']
[2024-06-12T12:33:39.815+0000] {standard_task_runner.py:91} INFO - Job 34: Subtask fetch_and_publish_driver
[2024-06-12T12:33:39.851+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host 79a7d5bd11df
[2024-06-12T12:33:39.908+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T12:33:39.909+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T12:33:41.392+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T12:33:41.394+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: connecting to kafka:9092 [('172.25.0.6', 9092) IPv4]
[2024-06-12T12:33:41.394+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T12:33:41.395+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: Connection complete.
[2024-06-12T12:33:41.497+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T12:33:41.498+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T12:33:41.652+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: connecting to kafka:9092 [('172.25.0.6', 9092) IPv4]
[2024-06-12T12:33:41.652+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: Connection complete.
[2024-06-12T12:33:41.653+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.25.0.6', 9092)]>: Closing connection. 
[2024-06-12T12:33:41.766+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T12:33:41.770+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.25.0.6', 9092)]>: Closing connection. 
[2024-06-12T12:33:41.770+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T12:33:41.770+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T12:33:41.777+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T123339, end_date=20240612T123341
[2024-06-12T12:33:41.793+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T12:33:41.806+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-12T12:33:41.808+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T12:40:41.661+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T12:40:41.678+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T12:40:41.684+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T12:40:41.685+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T12:40:41.693+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T12:40:41.699+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=802) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T12:40:41.700+0000] {standard_task_runner.py:63} INFO - Started process 811 to run task
[2024-06-12T12:40:41.700+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '53', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmplqji06xd']
[2024-06-12T12:40:41.702+0000] {standard_task_runner.py:91} INFO - Job 53: Subtask fetch_and_publish_driver
[2024-06-12T12:40:41.741+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host 79a7d5bd11df
[2024-06-12T12:40:41.800+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T12:40:41.801+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T12:40:43.304+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T12:40:43.306+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: connecting to kafka:9092 [('172.25.0.6', 9092) IPv4]
[2024-06-12T12:40:43.306+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T12:40:43.307+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: Connection complete.
[2024-06-12T12:40:43.409+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T12:40:43.409+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T12:40:43.412+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T12:40:43.414+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: connecting to kafka:9092 [('172.25.0.6', 9092) IPv4]
[2024-06-12T12:40:43.414+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: Connection complete.
[2024-06-12T12:40:43.414+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.25.0.6', 9092)]>: Closing connection. 
[2024-06-12T12:40:43.417+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.25.0.6', 9092)]>: Closing connection. 
[2024-06-12T12:40:43.418+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T12:40:43.418+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T12:40:43.426+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T124041, end_date=20240612T124043
[2024-06-12T12:40:43.439+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T12:40:43.446+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T12:57:18.468+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T12:57:18.486+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T12:57:18.494+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T12:57:18.494+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T12:57:18.503+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T12:57:18.508+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=618) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T12:57:18.509+0000] {standard_task_runner.py:63} INFO - Started process 626 to run task
[2024-06-12T12:57:18.510+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmpyzo7fes6']
[2024-06-12T12:57:18.511+0000] {standard_task_runner.py:91} INFO - Job 32: Subtask fetch_and_publish_driver
[2024-06-12T12:57:18.545+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host ce972f89b6ce
[2024-06-12T12:57:18.603+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T12:57:18.604+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T12:57:18.696+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T12:57:18.697+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.26.0.6', 9092)]>: connecting to kafka:9092 [('172.26.0.6', 9092) IPv4]
[2024-06-12T12:57:18.698+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T12:57:18.698+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.26.0.6', 9092)]>: Connection complete.
[2024-06-12T12:57:18.804+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T12:57:18.804+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T12:57:18.960+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.26.0.6', 9092)]>: connecting to kafka:9092 [('172.26.0.6', 9092) IPv4]
[2024-06-12T12:57:18.960+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.26.0.6', 9092)]>: Connection complete.
[2024-06-12T12:57:18.961+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.26.0.6', 9092)]>: Closing connection. 
[2024-06-12T12:57:19.085+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T12:57:19.090+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.26.0.6', 9092)]>: Closing connection. 
[2024-06-12T12:57:19.091+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T12:57:19.091+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T12:57:19.098+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T125718, end_date=20240612T125719
[2024-06-12T12:57:19.124+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T12:57:19.139+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-12T12:57:19.140+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T13:12:08.010+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T13:12:08.027+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T13:12:08.033+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T13:12:08.033+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T13:12:08.042+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T13:12:08.046+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=596) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T13:12:08.047+0000] {standard_task_runner.py:63} INFO - Started process 601 to run task
[2024-06-12T13:12:08.048+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '35', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmpt6o2zdzv']
[2024-06-12T13:12:08.049+0000] {standard_task_runner.py:91} INFO - Job 35: Subtask fetch_and_publish_driver
[2024-06-12T13:12:08.085+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host 75496fb8ac8a
[2024-06-12T13:12:08.142+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T13:12:08.142+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T13:12:09.792+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T13:12:09.793+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.28.0.6', 9092)]>: connecting to kafka:9092 [('172.28.0.6', 9092) IPv4]
[2024-06-12T13:12:09.794+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T13:12:09.794+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.28.0.6', 9092)]>: Connection complete.
[2024-06-12T13:12:09.897+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T13:12:09.897+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T13:12:10.051+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.28.0.6', 9092)]>: connecting to kafka:9092 [('172.28.0.6', 9092) IPv4]
[2024-06-12T13:12:10.052+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.28.0.6', 9092)]>: Connection complete.
[2024-06-12T13:12:10.052+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.28.0.6', 9092)]>: Closing connection. 
[2024-06-12T13:12:10.702+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T13:12:10.849+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.28.0.6', 9092)]>: Closing connection. 
[2024-06-12T13:12:10.849+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T13:12:10.850+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T13:12:10.857+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T131208, end_date=20240612T131210
[2024-06-12T13:12:10.910+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T13:12:10.917+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T13:25:36.892+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T13:25:36.910+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T13:25:36.917+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T13:25:36.917+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T13:25:36.925+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T13:25:36.930+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=605) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T13:25:36.931+0000] {standard_task_runner.py:63} INFO - Started process 614 to run task
[2024-06-12T13:25:36.931+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '36', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmp5_hnffaw']
[2024-06-12T13:25:36.933+0000] {standard_task_runner.py:91} INFO - Job 36: Subtask fetch_and_publish_driver
[2024-06-12T13:25:36.968+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host 7ad193764747
[2024-06-12T13:25:37.025+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T13:25:37.026+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T13:25:38.203+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T13:25:38.205+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.29.0.6', 9092)]>: connecting to kafka:9092 [('172.29.0.6', 9092) IPv4]
[2024-06-12T13:25:38.205+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T13:25:38.206+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.29.0.6', 9092)]>: Connection complete.
[2024-06-12T13:25:38.308+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T13:25:38.309+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T13:25:38.464+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.29.0.6', 9092)]>: connecting to kafka:9092 [('172.29.0.6', 9092) IPv4]
[2024-06-12T13:25:38.464+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.29.0.6', 9092)]>: Connection complete.
[2024-06-12T13:25:38.464+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.29.0.6', 9092)]>: Closing connection. 
[2024-06-12T13:25:38.579+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T13:25:38.585+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.29.0.6', 9092)]>: Closing connection. 
[2024-06-12T13:25:38.586+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T13:25:38.586+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T13:25:38.593+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T132536, end_date=20240612T132538
[2024-06-12T13:25:38.630+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T13:25:38.643+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-12T13:25:38.644+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-12T13:32:20.510+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-12T13:32:20.529+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T13:32:20.536+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [queued]>
[2024-06-12T13:32:20.536+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-12T13:32:20.545+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_driver> on 2024-06-11 00:00:00+00:00
[2024-06-12T13:32:20.549+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=590) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-12T13:32:20.550+0000] {standard_task_runner.py:63} INFO - Started process 599 to run task
[2024-06-12T13:32:20.551+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow_with_kafka', 'fetch_and_publish_driver', 'scheduled__2024-06-11T00:00:00+00:00', '--job-id', '36', '--raw', '--subdir', 'DAGS_FOLDER/kafka_data_scrape.py', '--cfg-path', '/tmp/tmp4yxrblec']
[2024-06-12T13:32:20.552+0000] {standard_task_runner.py:91} INFO - Job 36: Subtask fetch_and_publish_driver
[2024-06-12T13:32:20.587+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow_with_kafka.fetch_and_publish_driver scheduled__2024-06-11T00:00:00+00:00 [running]> on host c05e5b3efdbc
[2024-06-12T13:32:20.646+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow_with_kafka' AIRFLOW_CTX_TASK_ID='fetch_and_publish_driver' AIRFLOW_CTX_EXECUTION_DATE='2024-06-11T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-11T00:00:00+00:00'
[2024-06-12T13:32:20.647+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-12T13:32:22.782+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-12T13:32:22.784+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.30.0.6', 9092)]>: connecting to kafka:9092 [('172.30.0.6', 9092) IPv4]
[2024-06-12T13:32:22.784+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-12T13:32:22.785+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.30.0.6', 9092)]>: Connection complete.
[2024-06-12T13:32:22.887+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-12T13:32:22.887+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-12T13:32:23.041+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.30.0.6', 9092)]>: connecting to kafka:9092 [('172.30.0.6', 9092) IPv4]
[2024-06-12T13:32:23.042+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.30.0.6', 9092)]>: Connection complete.
[2024-06-12T13:32:23.042+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.30.0.6', 9092)]>: Closing connection. 
[2024-06-12T13:32:23.160+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-12T13:32:23.164+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.30.0.6', 9092)]>: Closing connection. 
[2024-06-12T13:32:23.165+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-12T13:32:23.165+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-12T13:32:23.172+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow_with_kafka, task_id=fetch_and_publish_driver, run_id=scheduled__2024-06-11T00:00:00+00:00, execution_date=20240611T000000, start_date=20240612T133220, end_date=20240612T133223
[2024-06-12T13:32:23.213+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-12T13:32:23.226+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-12T13:32:23.227+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
