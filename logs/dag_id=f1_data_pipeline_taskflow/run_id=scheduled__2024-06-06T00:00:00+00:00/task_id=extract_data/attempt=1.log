[2024-06-07T08:42:14.762+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T08:42:14.777+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T08:42:14.782+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T08:42:14.782+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T08:42:14.790+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T08:42:14.794+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=95) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T08:42:14.795+0000] {standard_task_runner.py:63} INFO - Started process 97 to run task
[2024-06-07T08:42:14.795+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '43', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpikbpfdpx']
[2024-06-07T08:42:14.797+0000] {standard_task_runner.py:91} INFO - Job 43: Subtask extract_data
[2024-06-07T08:42:14.829+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host 9459748fcbed
[2024-06-07T08:42:14.885+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T08:42:14.886+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T08:42:17.823+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T08:42:42.727+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T08:42:42.792+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T08:42:44.236+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T084214, end_date=20240607T084244
[2024-06-07T08:42:44.275+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T08:42:44.302+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T08:42:44.304+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-07T10:22:09.418+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T10:22:09.431+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T10:22:09.435+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T10:22:09.435+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T10:22:09.442+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T10:22:09.446+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=103) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T10:22:09.447+0000] {standard_task_runner.py:63} INFO - Started process 105 to run task
[2024-06-07T10:22:09.447+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp6s_9msrq']
[2024-06-07T10:22:09.449+0000] {standard_task_runner.py:91} INFO - Job 7: Subtask extract_data
[2024-06-07T10:22:09.482+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host c52e0424619c
[2024-06-07T10:22:09.535+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T10:22:09.536+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T10:22:12.386+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T10:22:37.969+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T10:22:38.033+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T10:22:39.468+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T102209, end_date=20240607T102239
[2024-06-07T10:22:39.531+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T10:22:39.556+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T10:22:39.558+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-07T10:28:39.818+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T10:28:39.833+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T10:28:39.839+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T10:28:39.839+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T10:28:39.849+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T10:28:39.853+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T10:28:39.854+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-07T10:28:39.855+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpgos9og7c']
[2024-06-07T10:28:39.857+0000] {standard_task_runner.py:91} INFO - Job 5: Subtask extract_data
[2024-06-07T10:28:40.000+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host 560daa7ebc6f
[2024-06-07T10:28:40.050+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T10:28:40.050+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T10:28:43.019+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T10:29:08.083+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T10:29:08.145+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T10:29:09.584+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T102839, end_date=20240607T102909
[2024-06-07T10:29:09.616+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T10:29:09.641+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T10:29:09.642+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-07T10:54:15.327+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T10:54:15.340+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T10:54:15.345+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T10:54:15.345+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T10:54:15.352+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T10:54:15.356+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=194) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T10:54:15.356+0000] {standard_task_runner.py:63} INFO - Started process 196 to run task
[2024-06-07T10:54:15.357+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpnz39yzhm']
[2024-06-07T10:54:15.358+0000] {standard_task_runner.py:91} INFO - Job 4: Subtask extract_data
[2024-06-07T10:54:15.390+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host 511ae3a85ee7
[2024-06-07T10:54:15.447+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T10:54:15.448+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T10:54:18.434+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T10:54:44.054+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T10:54:44.116+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T10:54:45.641+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T105415, end_date=20240607T105445
[2024-06-07T10:54:45.667+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T10:54:45.690+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T10:54:45.691+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-07T11:03:43.793+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T11:03:43.808+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T11:03:43.813+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T11:03:43.813+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T11:03:43.822+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T11:03:43.826+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=74) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T11:03:43.828+0000] {standard_task_runner.py:63} INFO - Started process 76 to run task
[2024-06-07T11:03:43.827+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpbqevz6yh']
[2024-06-07T11:03:43.829+0000] {standard_task_runner.py:91} INFO - Job 5: Subtask extract_data
[2024-06-07T11:03:43.970+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host 56deb138ba11
[2024-06-07T11:03:44.027+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T11:03:44.027+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T11:03:46.942+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T11:04:12.558+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T11:04:12.630+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T11:04:14.051+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T110343, end_date=20240607T110414
[2024-06-07T11:04:14.077+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T11:04:14.100+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T11:04:14.102+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-07T11:14:29.140+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T11:14:29.154+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T11:14:29.158+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T11:14:29.158+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T11:14:29.166+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T11:14:29.170+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=102) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T11:14:29.171+0000] {standard_task_runner.py:63} INFO - Started process 104 to run task
[2024-06-07T11:14:29.171+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpxsi8euij']
[2024-06-07T11:14:29.173+0000] {standard_task_runner.py:91} INFO - Job 5: Subtask extract_data
[2024-06-07T11:14:29.313+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host bdba731e3ece
[2024-06-07T11:14:29.369+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T11:14:29.370+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T11:14:32.279+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T11:14:57.022+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T11:14:57.082+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T11:14:58.533+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T111429, end_date=20240607T111458
[2024-06-07T11:14:58.569+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T11:14:58.592+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T11:14:58.594+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-07T12:30:19.525+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T12:30:19.540+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T12:30:19.544+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T12:30:19.544+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T12:30:19.552+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T12:30:19.557+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T12:30:19.558+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-07T12:30:19.558+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp44lhqovj']
[2024-06-07T12:30:19.559+0000] {standard_task_runner.py:91} INFO - Job 5: Subtask extract_data
[2024-06-07T12:30:19.699+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host 5a6477355f45
[2024-06-07T12:30:19.750+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T12:30:19.751+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T12:30:22.598+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:41: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T12:30:48.175+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T12:30:48.233+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T12:30:49.659+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T123019, end_date=20240607T123049
[2024-06-07T12:30:49.714+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T12:30:49.736+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T12:30:49.738+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-07T13:01:39.002+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T13:01:39.016+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T13:01:39.020+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T13:01:39.020+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T13:01:39.028+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T13:01:39.033+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T13:01:39.033+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-07T13:01:39.034+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmprc9f55zk']
[2024-06-07T13:01:39.035+0000] {standard_task_runner.py:91} INFO - Job 5: Subtask extract_data
[2024-06-07T13:01:39.179+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host f698bc001b17
[2024-06-07T13:01:39.230+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T13:01:39.231+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T13:01:42.089+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:41: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T13:02:07.273+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T13:02:07.337+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T13:02:08.790+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T130139, end_date=20240607T130208
[2024-06-07T13:02:08.828+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T13:02:08.853+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T13:02:08.854+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-07T13:10:05.907+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T13:10:05.921+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T13:10:05.926+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T13:10:05.926+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T13:10:05.934+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T13:10:05.939+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T13:10:05.939+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-07T13:10:05.940+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpxi9ssox7']
[2024-06-07T13:10:05.941+0000] {standard_task_runner.py:91} INFO - Job 5: Subtask extract_data
[2024-06-07T13:10:06.083+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host 5220b11abc3b
[2024-06-07T13:10:06.133+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T13:10:06.134+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T13:10:08.953+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:41: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T13:10:34.173+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T13:10:34.233+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T13:10:35.683+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T131005, end_date=20240607T131035
[2024-06-07T13:10:35.749+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T13:10:35.771+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T13:10:35.774+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-07T14:13:16.733+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T14:13:16.747+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T14:13:16.752+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T14:13:16.752+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T14:13:16.761+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T14:13:16.766+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=68) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T14:13:16.767+0000] {standard_task_runner.py:63} INFO - Started process 70 to run task
[2024-06-07T14:13:16.767+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpq18hb8qc']
[2024-06-07T14:13:16.768+0000] {standard_task_runner.py:91} INFO - Job 5: Subtask extract_data
[2024-06-07T14:13:16.910+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host 21a09bc8bac4
[2024-06-07T14:13:16.963+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T14:13:16.964+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T14:13:19.952+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:41: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T14:13:44.876+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T14:13:44.935+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T14:13:46.374+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T141316, end_date=20240607T141346
[2024-06-07T14:13:46.433+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T14:13:46.460+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T14:13:46.462+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-07T14:41:07.101+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-07T14:41:07.117+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T14:41:07.122+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [queued]>
[2024-06-07T14:41:07.122+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-07T14:41:07.132+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-06 00:00:00+00:00
[2024-06-07T14:41:07.137+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-07T14:41:07.139+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-07T14:41:07.139+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-06T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpca00t9tk']
[2024-06-07T14:41:07.140+0000] {standard_task_runner.py:91} INFO - Job 5: Subtask extract_data
[2024-06-07T14:41:07.281+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-06T00:00:00+00:00 [running]> on host 5b44fb81e12c
[2024-06-07T14:41:07.330+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-06T00:00:00+00:00'
[2024-06-07T14:41:07.331+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-07T14:41:10.271+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:41: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-07T14:41:35.381+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-07T14:41:35.451+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-07T14:41:36.863+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-06T00:00:00+00:00, execution_date=20240606T000000, start_date=20240607T144107, end_date=20240607T144136
[2024-06-07T14:41:36.927+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-07T14:41:36.951+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-07T14:41:36.953+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
