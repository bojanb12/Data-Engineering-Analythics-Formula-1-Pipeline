[2024-06-19T07:28:10.851+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T07:28:10.873+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T07:28:10.879+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T07:28:10.879+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T07:28:10.889+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T07:28:10.893+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=107) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T07:28:10.894+0000] {standard_task_runner.py:63} INFO - Started process 112 to run task
[2024-06-19T07:28:10.895+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp04x5f_uo']
[2024-06-19T07:28:10.896+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T07:28:10.929+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host 5d5bc16a80d8
[2024-06-19T07:28:10.983+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T07:28:10.984+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T07:28:13.864+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T07:28:38.680+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T07:28:38.736+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T07:28:40.223+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T072810, end_date=20240619T072840
[2024-06-19T07:28:40.248+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T07:28:40.277+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T07:28:40.279+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T08:26:32.784+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T08:26:32.806+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T08:26:32.811+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T08:26:32.811+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T08:26:32.820+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T08:26:32.824+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=78) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T08:26:32.825+0000] {standard_task_runner.py:63} INFO - Started process 82 to run task
[2024-06-19T08:26:32.825+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmptw17xbbn']
[2024-06-19T08:26:32.826+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T08:26:32.989+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host d64590577fb7
[2024-06-19T08:26:33.042+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T08:26:33.043+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T08:26:35.940+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T08:27:00.497+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T08:27:00.549+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T08:27:01.961+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T082632, end_date=20240619T082701
[2024-06-19T08:27:02.013+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T08:27:02.040+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T08:27:02.042+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T08:53:39.159+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T08:53:39.183+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T08:53:39.189+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T08:53:39.189+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T08:53:39.198+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T08:53:39.203+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=77) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T08:53:39.204+0000] {standard_task_runner.py:63} INFO - Started process 83 to run task
[2024-06-19T08:53:39.204+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp4f5or7bn']
[2024-06-19T08:53:39.205+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T08:53:39.251+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host e5c8778eab06
[2024-06-19T08:53:39.305+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T08:53:39.306+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T08:53:42.325+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T08:54:07.359+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T08:54:07.411+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T08:54:08.820+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T085339, end_date=20240619T085408
[2024-06-19T08:54:08.868+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T08:54:08.890+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T08:54:08.892+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T12:04:55.809+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T12:04:55.834+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:04:55.839+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:04:55.839+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T12:04:55.848+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T12:04:55.853+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=87) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T12:04:55.854+0000] {standard_task_runner.py:63} INFO - Started process 91 to run task
[2024-06-19T12:04:55.854+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp9mjnbdjp']
[2024-06-19T12:04:55.856+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T12:04:55.893+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host eef124934bf8
[2024-06-19T12:04:55.952+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T12:04:55.953+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T12:04:58.470+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T12:05:24.093+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T12:05:24.151+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T12:05:25.571+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T120455, end_date=20240619T120525
[2024-06-19T12:05:25.629+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T12:05:25.652+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T12:05:25.654+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T12:36:05.852+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T12:36:05.874+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:36:05.878+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:36:05.878+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T12:36:05.886+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T12:36:05.890+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=120) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T12:36:05.891+0000] {standard_task_runner.py:63} INFO - Started process 122 to run task
[2024-06-19T12:36:05.891+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpepat44vh']
[2024-06-19T12:36:05.893+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T12:36:06.036+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host 3a98e934d90a
[2024-06-19T12:36:06.089+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T12:36:06.090+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T12:36:09.034+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T12:36:34.528+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T12:36:34.583+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T12:36:36.074+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T123605, end_date=20240619T123636
[2024-06-19T12:36:36.112+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T12:36:36.119+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T12:52:35.549+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T12:52:35.570+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:52:35.576+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:52:35.576+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T12:52:35.584+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T12:52:35.589+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=79) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T12:52:35.590+0000] {standard_task_runner.py:63} INFO - Started process 83 to run task
[2024-06-19T12:52:35.590+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp_2kjm4px']
[2024-06-19T12:52:35.592+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T12:52:35.757+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host 9426bfe013d2
[2024-06-19T12:52:35.810+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T12:52:35.811+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T12:52:38.723+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T12:53:04.456+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T12:53:04.509+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T12:53:05.918+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T125235, end_date=20240619T125305
[2024-06-19T12:53:05.973+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T12:53:06.002+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T12:53:06.005+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:02:37.449+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:02:37.473+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:02:37.478+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:02:37.478+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T13:02:37.489+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:02:37.494+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=78) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T13:02:37.496+0000] {standard_task_runner.py:63} INFO - Started process 82 to run task
[2024-06-19T13:02:37.496+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpscxy706j']
[2024-06-19T13:02:37.498+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T13:02:37.535+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host 6fa64189ef34
[2024-06-19T13:02:37.595+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:02:37.596+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:02:40.565+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T13:03:06.283+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T13:03:06.348+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:03:07.835+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T130237, end_date=20240619T130307
[2024-06-19T13:03:07.897+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:03:07.920+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:03:07.921+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:16:23.220+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:16:23.243+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:16:23.249+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:16:23.250+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T13:16:23.261+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:16:23.266+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=79) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T13:16:23.268+0000] {standard_task_runner.py:63} INFO - Started process 82 to run task
[2024-06-19T13:16:23.268+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpiyu60m5p']
[2024-06-19T13:16:23.270+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T13:16:23.309+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host d5b44298ec9f
[2024-06-19T13:16:23.369+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:16:23.370+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:16:26.330+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T13:16:52.084+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T13:16:52.141+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:16:53.592+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T131623, end_date=20240619T131653
[2024-06-19T13:16:53.658+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:16:53.680+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:16:53.682+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:26:51.967+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:26:51.998+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:26:52.009+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:26:52.009+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T13:26:52.026+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:26:52.034+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T13:26:52.036+0000] {standard_task_runner.py:63} INFO - Started process 77 to run task
[2024-06-19T13:26:52.035+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpenclyr14']
[2024-06-19T13:26:52.038+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T13:26:52.097+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host fc0cf0aa1225
[2024-06-19T13:26:52.165+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:26:52.165+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:26:55.092+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T13:27:20.536+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T13:27:20.591+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:27:22.027+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T132651, end_date=20240619T132722
[2024-06-19T13:27:22.060+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:27:22.083+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:27:22.084+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:35:21.513+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:35:21.537+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:35:21.543+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:35:21.543+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T13:35:21.682+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:35:21.687+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=64) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T13:35:21.688+0000] {standard_task_runner.py:63} INFO - Started process 75 to run task
[2024-06-19T13:35:21.688+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpcxxygype']
[2024-06-19T13:35:21.690+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T13:35:21.729+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host a7ca02a49f0a
[2024-06-19T13:35:21.795+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:35:21.796+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:35:24.794+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T13:35:51.354+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T13:35:51.412+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:35:52.947+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T133521, end_date=20240619T133552
[2024-06-19T13:35:53.011+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:35:53.039+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:35:53.041+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:37:00.536+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:37:00.559+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:37:00.565+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:37:00.565+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T13:37:00.576+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:37:00.580+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=64) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T13:37:00.581+0000] {standard_task_runner.py:63} INFO - Started process 66 to run task
[2024-06-19T13:37:00.581+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp2067vho7']
[2024-06-19T13:37:00.583+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T13:37:00.742+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host 790bccf90551
[2024-06-19T13:37:00.797+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:37:00.798+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:37:03.409+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T13:37:28.684+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T13:37:28.740+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:37:30.185+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T133700, end_date=20240619T133730
[2024-06-19T13:37:30.224+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:37:30.247+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:37:30.248+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T14:00:45.721+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T14:00:45.743+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:00:45.748+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:00:45.748+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T14:00:45.756+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T14:00:45.760+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T14:00:45.761+0000] {standard_task_runner.py:63} INFO - Started process 75 to run task
[2024-06-19T14:00:45.762+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmploqdkgek']
[2024-06-19T14:00:45.763+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T14:00:45.806+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host 0ff0d3189758
[2024-06-19T14:00:45.870+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T14:00:45.871+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T14:00:48.748+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T14:01:13.611+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T14:01:13.665+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T14:01:15.082+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T140045, end_date=20240619T140115
[2024-06-19T14:01:15.121+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T14:01:15.144+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T14:01:15.146+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T14:04:23.256+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T14:04:23.273+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:04:23.278+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:04:23.278+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T14:04:23.287+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T14:04:23.292+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=64) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T14:04:23.293+0000] {standard_task_runner.py:63} INFO - Started process 79 to run task
[2024-06-19T14:04:23.293+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpepzos8yl']
[2024-06-19T14:04:23.296+0000] {standard_task_runner.py:91} INFO - Job 4: Subtask extract_data
[2024-06-19T14:04:23.341+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host dc92dd71b4a2
[2024-06-19T14:04:23.402+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T14:04:23.403+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T14:04:25.924+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T14:04:50.638+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T14:04:50.689+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T14:04:52.082+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T140423, end_date=20240619T140452
[2024-06-19T14:04:52.112+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T14:04:52.133+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T14:04:52.135+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T14:13:22.384+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T14:13:22.406+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:13:22.410+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:13:22.410+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T14:13:22.420+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T14:13:22.424+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=98) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T14:13:22.425+0000] {standard_task_runner.py:63} INFO - Started process 102 to run task
[2024-06-19T14:13:22.425+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpq88rer83']
[2024-06-19T14:13:22.426+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T14:13:22.599+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host 2acd74b25e37
[2024-06-19T14:13:22.654+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T14:13:22.654+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T14:13:25.517+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T14:13:50.514+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T14:13:50.584+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T14:13:52.065+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T141322, end_date=20240619T141352
[2024-06-19T14:13:52.103+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T14:13:52.123+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T14:13:52.125+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T14:25:19.333+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T14:25:19.355+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:25:19.360+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:25:19.360+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T14:25:19.370+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-18 00:00:00+00:00
[2024-06-19T14:25:19.375+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=72) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T14:25:19.376+0000] {standard_task_runner.py:63} INFO - Started process 75 to run task
[2024-06-19T14:25:19.376+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpuvo27g8n']
[2024-06-19T14:25:19.378+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-19T14:25:19.418+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-18T00:00:00+00:00 [running]> on host 7f714baf6013
[2024-06-19T14:25:19.476+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T14:25:19.476+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T14:25:22.397+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-19T14:25:47.566+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-19T14:25:47.620+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T14:25:49.121+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T142519, end_date=20240619T142549
[2024-06-19T14:25:49.195+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T14:25:49.221+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-19T14:25:49.224+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
