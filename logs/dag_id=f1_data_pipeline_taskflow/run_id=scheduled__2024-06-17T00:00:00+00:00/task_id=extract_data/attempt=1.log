[2024-06-18T07:19:21.034+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T07:19:21.056+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T07:19:21.060+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T07:19:21.061+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T07:19:21.069+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T07:19:21.074+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T07:19:21.075+0000] {standard_task_runner.py:63} INFO - Started process 73 to run task
[2024-06-18T07:19:21.075+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpn42g59lm']
[2024-06-18T07:19:21.077+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T07:19:21.236+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host ce62f6b650cd
[2024-06-18T07:19:21.286+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T07:19:21.286+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T07:19:24.170+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T07:19:49.261+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T07:19:49.314+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T07:19:50.854+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T071921, end_date=20240618T071950
[2024-06-18T07:19:50.879+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T07:19:50.901+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T07:19:50.903+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T08:11:29.714+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T08:11:29.735+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T08:11:29.739+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T08:11:29.739+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T08:11:29.747+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T08:11:29.752+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T08:11:29.753+0000] {standard_task_runner.py:63} INFO - Started process 75 to run task
[2024-06-18T08:11:29.753+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp17tcn3s9']
[2024-06-18T08:11:29.754+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T08:11:29.917+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host 0a8bae48237c
[2024-06-18T08:11:29.971+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T08:11:29.972+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T08:11:32.489+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T08:11:57.268+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T08:11:57.317+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T08:11:58.758+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T081129, end_date=20240618T081158
[2024-06-18T08:11:58.790+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T08:11:58.821+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T08:11:58.823+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T08:22:07.731+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T08:22:07.753+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T08:22:07.759+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T08:22:07.759+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T08:22:07.767+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T08:22:07.771+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=113) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T08:22:07.772+0000] {standard_task_runner.py:63} INFO - Started process 115 to run task
[2024-06-18T08:22:07.773+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpg7mpy3gy']
[2024-06-18T08:22:07.774+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T08:22:07.914+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host 7d3c9e9f45b7
[2024-06-18T08:22:07.963+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T08:22:07.964+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T08:22:10.797+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T08:22:35.727+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T08:22:35.779+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T08:22:37.214+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T082207, end_date=20240618T082237
[2024-06-18T08:22:37.251+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T08:22:37.273+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T08:22:37.274+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T08:29:33.301+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T08:29:33.322+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T08:29:33.327+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T08:29:33.327+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T08:29:33.335+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T08:29:33.339+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T08:29:33.340+0000] {standard_task_runner.py:63} INFO - Started process 75 to run task
[2024-06-18T08:29:33.340+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpkllfwfgj']
[2024-06-18T08:29:33.342+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T08:29:33.506+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host d26fdf24870c
[2024-06-18T08:29:33.557+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T08:29:33.558+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T08:29:36.463+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T08:30:01.349+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T08:30:01.400+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T08:30:02.887+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T082933, end_date=20240618T083002
[2024-06-18T08:30:02.931+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T08:30:02.954+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T08:30:02.955+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T08:54:06.326+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T08:54:06.347+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T08:54:06.351+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T08:54:06.351+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T08:54:06.359+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T08:54:06.364+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=85) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T08:54:06.365+0000] {standard_task_runner.py:63} INFO - Started process 89 to run task
[2024-06-18T08:54:06.365+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmphew9bvtz']
[2024-06-18T08:54:06.366+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T08:54:06.403+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host 70dbe2d93c7b
[2024-06-18T08:54:06.454+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T08:54:06.456+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T08:54:09.160+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T08:54:34.797+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T08:54:34.872+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T08:54:36.133+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T085406, end_date=20240618T085436
[2024-06-18T08:54:36.178+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T08:54:36.200+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T08:54:36.202+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T08:57:38.745+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T08:57:38.765+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T08:57:38.769+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T08:57:38.769+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T08:57:38.778+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T08:57:38.782+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T08:57:38.783+0000] {standard_task_runner.py:63} INFO - Started process 73 to run task
[2024-06-18T08:57:38.783+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpv20bahlk']
[2024-06-18T08:57:38.785+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T08:57:38.945+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host 85d13f87db98
[2024-06-18T08:57:38.998+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T08:57:38.998+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T08:57:41.900+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T08:58:06.821+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T08:58:06.871+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T08:58:08.301+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T085738, end_date=20240618T085808
[2024-06-18T08:58:08.355+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T08:58:08.376+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T08:58:08.378+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T11:55:02.107+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T11:55:02.128+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T11:55:02.133+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T11:55:02.133+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T11:55:02.143+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T11:55:02.148+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=78) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T11:55:02.149+0000] {standard_task_runner.py:63} INFO - Started process 80 to run task
[2024-06-18T11:55:02.149+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpjjfhwsdq']
[2024-06-18T11:55:02.151+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T11:55:02.308+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host 21d7efd887f9
[2024-06-18T11:55:02.360+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T11:55:02.360+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T11:55:05.003+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T11:55:29.134+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T11:55:29.186+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T11:55:30.616+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T115502, end_date=20240618T115530
[2024-06-18T11:55:30.675+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T11:55:30.703+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T11:55:30.705+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T12:52:06.736+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T12:52:06.757+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T12:52:06.762+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T12:52:06.762+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T12:52:06.772+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T12:52:06.776+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=84) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T12:52:06.777+0000] {standard_task_runner.py:63} INFO - Started process 86 to run task
[2024-06-18T12:52:06.777+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpyzk24155']
[2024-06-18T12:52:06.778+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T12:52:06.925+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host 29634c3e6318
[2024-06-18T12:52:06.979+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T12:52:06.980+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T12:52:09.889+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T12:52:35.123+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T12:52:35.178+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T12:52:36.611+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T125206, end_date=20240618T125236
[2024-06-18T12:52:36.652+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T12:52:36.673+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T12:52:36.675+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T13:17:37.244+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T13:17:37.268+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T13:17:37.273+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T13:17:37.273+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T13:17:37.282+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T13:17:37.286+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=100) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T13:17:37.287+0000] {standard_task_runner.py:63} INFO - Started process 105 to run task
[2024-06-18T13:17:37.287+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpzcfupbob']
[2024-06-18T13:17:37.289+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T13:17:37.326+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host 642827d53473
[2024-06-18T13:17:37.380+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T13:17:37.381+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T13:17:39.927+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T13:18:04.784+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T13:18:04.848+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T13:18:06.293+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T131737, end_date=20240618T131806
[2024-06-18T13:18:06.346+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T13:18:06.367+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T13:18:06.373+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T13:27:46.954+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T13:27:46.977+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T13:27:46.982+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T13:27:46.982+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T13:27:46.992+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T13:27:46.996+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=77) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T13:27:46.997+0000] {standard_task_runner.py:63} INFO - Started process 83 to run task
[2024-06-18T13:27:46.997+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp4aw8yxji']
[2024-06-18T13:27:46.999+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T13:27:47.166+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host 9ae4be1c5479
[2024-06-18T13:27:47.218+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T13:27:47.218+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T13:27:50.168+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T13:28:15.206+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T13:28:15.258+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T13:28:16.726+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T132746, end_date=20240618T132816
[2024-06-18T13:28:16.756+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T13:28:16.780+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T13:28:16.781+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T13:56:58.507+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T13:56:58.529+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T13:56:58.534+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T13:56:58.535+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T13:56:58.544+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T13:56:58.548+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=99) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T13:56:58.549+0000] {standard_task_runner.py:63} INFO - Started process 103 to run task
[2024-06-18T13:56:58.549+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpg3i9xgww']
[2024-06-18T13:56:58.550+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T13:56:58.712+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host a16dbee39cfe
[2024-06-18T13:56:58.762+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T13:56:58.763+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T13:57:01.607+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T13:57:26.733+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T13:57:26.801+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T13:57:28.248+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T135658, end_date=20240618T135728
[2024-06-18T13:57:28.297+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T13:57:28.319+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T13:57:28.321+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-18T14:06:33.799+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-18T14:06:33.826+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T14:06:33.831+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [queued]>
[2024-06-18T14:06:33.831+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-18T14:06:33.840+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-17 00:00:00+00:00
[2024-06-18T14:06:33.847+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-17T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp46ffir10']
[2024-06-18T14:06:33.847+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=85) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-18T14:06:33.849+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-18T14:06:33.849+0000] {standard_task_runner.py:63} INFO - Started process 91 to run task
[2024-06-18T14:06:33.890+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-17T00:00:00+00:00 [running]> on host f083cbcc503e
[2024-06-18T14:06:33.949+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-17T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-17T00:00:00+00:00'
[2024-06-18T14:06:33.950+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-18T14:06:36.885+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-18T14:07:01.611+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-18T14:07:01.662+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-18T14:07:03.087+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-17T00:00:00+00:00, execution_date=20240617T000000, start_date=20240618T140633, end_date=20240618T140703
[2024-06-18T14:07:03.146+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-18T14:07:03.167+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-18T14:07:03.169+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
