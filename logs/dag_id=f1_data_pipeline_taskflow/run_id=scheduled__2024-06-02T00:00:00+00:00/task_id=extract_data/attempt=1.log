[2024-06-03T07:14:03.353+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T07:14:03.367+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:14:03.372+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:14:03.372+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T07:14:03.382+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T07:14:03.386+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=123) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T07:14:03.387+0000] {standard_task_runner.py:63} INFO - Started process 132 to run task
[2024-06-03T07:14:03.387+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpzcssea1p']
[2024-06-03T07:14:03.388+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T07:14:03.529+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host be846d410314
[2024-06-03T07:14:03.582+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T07:14:03.583+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T07:14:06.465+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T07:14:32.114+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T07:14:32.166+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T07:14:33.618+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T071403, end_date=20240603T071433
[2024-06-03T07:14:33.662+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T07:14:33.679+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T07:14:33.681+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T07:26:06.548+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T07:26:06.561+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:26:06.566+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:26:06.566+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T07:26:06.574+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T07:26:06.578+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=74) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T07:26:06.578+0000] {standard_task_runner.py:63} INFO - Started process 76 to run task
[2024-06-03T07:26:06.579+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpvvuqchiy']
[2024-06-03T07:26:06.580+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T07:26:06.707+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host fe440ef43634
[2024-06-03T07:26:06.754+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T07:26:06.755+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T07:26:09.502+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T07:26:35.158+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T07:26:35.214+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T07:26:36.758+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T072606, end_date=20240603T072636
[2024-06-03T07:26:36.788+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T07:26:36.808+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T07:26:36.809+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T07:33:50.310+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T07:33:50.324+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:33:50.328+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:33:50.328+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T07:33:50.337+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T07:33:50.341+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=81) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T07:33:50.342+0000] {standard_task_runner.py:63} INFO - Started process 83 to run task
[2024-06-03T07:33:50.342+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpokq0w4zi']
[2024-06-03T07:33:50.343+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T07:33:50.481+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 031b99f2c772
[2024-06-03T07:33:50.526+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T07:33:50.527+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T07:33:53.333+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T07:34:19.058+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T07:34:19.115+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T07:34:20.567+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T073350, end_date=20240603T073420
[2024-06-03T07:34:20.598+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T07:34:20.618+0000] {taskinstance.py:3498} INFO - 11 downstream tasks scheduled from follow-on schedule check
[2024-06-03T07:34:20.619+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T07:40:27.717+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T07:40:27.731+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:40:27.736+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:40:27.736+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T07:40:27.851+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T07:40:27.856+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T07:40:27.857+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T07:40:27.856+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpvwu0jsto']
[2024-06-03T07:40:27.858+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T07:40:27.890+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 3d32ec53a04f
[2024-06-03T07:40:27.939+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T07:40:27.940+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T07:40:30.486+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T07:40:55.746+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T07:40:55.798+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T07:40:57.264+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T074027, end_date=20240603T074057
[2024-06-03T07:40:57.327+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T07:40:57.356+0000] {taskinstance.py:3498} INFO - 11 downstream tasks scheduled from follow-on schedule check
[2024-06-03T07:40:57.358+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T07:46:20.410+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T07:46:20.424+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:46:20.428+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:46:20.429+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T07:46:20.437+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T07:46:20.441+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=96) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T07:46:20.441+0000] {standard_task_runner.py:63} INFO - Started process 98 to run task
[2024-06-03T07:46:20.442+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpgmm0lmp9']
[2024-06-03T07:46:20.443+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T07:46:20.587+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 90dd42c8ae86
[2024-06-03T07:46:20.638+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T07:46:20.639+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T07:46:23.528+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T07:46:49.017+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T07:46:49.072+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T07:46:50.591+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T074620, end_date=20240603T074650
[2024-06-03T07:46:50.641+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T07:46:50.664+0000] {taskinstance.py:3498} INFO - 11 downstream tasks scheduled from follow-on schedule check
[2024-06-03T07:46:50.666+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T07:50:38.090+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T07:50:38.104+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:50:38.108+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:50:38.109+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T07:50:38.117+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T07:50:38.121+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T07:50:38.122+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T07:50:38.122+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpfnxs4rdk']
[2024-06-03T07:50:38.123+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T07:50:38.251+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 756af44f0e5b
[2024-06-03T07:50:38.297+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T07:50:38.298+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T07:50:41.199+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T07:51:06.763+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T07:51:06.820+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T07:51:08.297+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T075038, end_date=20240603T075108
[2024-06-03T07:51:08.322+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T07:51:08.341+0000] {taskinstance.py:3498} INFO - 11 downstream tasks scheduled from follow-on schedule check
[2024-06-03T07:51:08.342+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T07:53:55.712+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T07:53:55.728+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:53:55.734+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T07:53:55.734+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T07:53:55.744+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T07:53:55.748+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T07:53:55.750+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T07:53:55.750+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpdpgkssff']
[2024-06-03T07:53:55.752+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T07:53:55.885+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 233068ece505
[2024-06-03T07:53:55.941+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T07:53:55.942+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T07:53:58.849+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T07:54:24.059+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T07:54:24.114+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T07:54:25.567+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T075355, end_date=20240603T075425
[2024-06-03T07:54:25.595+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T07:54:25.616+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T07:54:25.618+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T08:06:47.436+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T08:06:47.454+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:06:47.459+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:06:47.459+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T08:06:47.566+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T08:06:47.571+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T08:06:47.572+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T08:06:47.572+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpblg80u_i']
[2024-06-03T08:06:47.574+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T08:06:47.611+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 93a1bd3d5710
[2024-06-03T08:06:47.667+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T08:06:47.668+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T08:06:50.664+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T08:07:15.490+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T08:07:15.544+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T08:07:16.998+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T080647, end_date=20240603T080716
[2024-06-03T08:07:17.032+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T08:07:17.056+0000] {taskinstance.py:3498} INFO - 11 downstream tasks scheduled from follow-on schedule check
[2024-06-03T08:07:17.058+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T08:09:45.382+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T08:09:45.414+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:09:45.419+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:09:45.419+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T08:09:45.517+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T08:09:45.521+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T08:09:45.522+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T08:09:45.523+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp9tg1fl3_']
[2024-06-03T08:09:45.524+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T08:09:45.555+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host f44292e0b246
[2024-06-03T08:09:45.604+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T08:09:45.605+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T08:09:48.535+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T08:10:14.189+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T08:10:14.242+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T08:10:15.674+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T080945, end_date=20240603T081015
[2024-06-03T08:10:15.728+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T08:10:15.750+0000] {taskinstance.py:3498} INFO - 12 downstream tasks scheduled from follow-on schedule check
[2024-06-03T08:10:15.751+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T08:13:24.754+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T08:13:24.769+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:13:24.774+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:13:24.775+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T08:13:24.873+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T08:13:24.877+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T08:13:24.878+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T08:13:24.878+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpb6ioomlw']
[2024-06-03T08:13:24.880+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T08:13:24.911+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 20366a32a365
[2024-06-03T08:13:24.959+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T08:13:24.959+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T08:13:27.874+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T08:13:53.064+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T08:13:53.121+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T08:13:54.589+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T081324, end_date=20240603T081354
[2024-06-03T08:13:54.641+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T08:13:54.662+0000] {taskinstance.py:3498} INFO - 12 downstream tasks scheduled from follow-on schedule check
[2024-06-03T08:13:54.664+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T08:16:32.347+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T08:16:32.362+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:16:32.367+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:16:32.367+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T08:16:32.473+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T08:16:32.477+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T08:16:32.479+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T08:16:32.479+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpoikdnljq']
[2024-06-03T08:16:32.480+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T08:16:32.515+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 2c74767b306e
[2024-06-03T08:16:32.569+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T08:16:32.569+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T08:16:35.647+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T08:17:00.880+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T08:17:00.936+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T08:17:02.387+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T081632, end_date=20240603T081702
[2024-06-03T08:17:02.449+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T08:17:02.472+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T08:17:02.474+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T08:33:08.115+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T08:33:08.130+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:33:08.135+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:33:08.136+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T08:33:08.240+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T08:33:08.244+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T08:33:08.245+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T08:33:08.245+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp6fbjvts5']
[2024-06-03T08:33:08.247+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T08:33:08.280+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 25a65b768c81
[2024-06-03T08:33:08.332+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T08:33:08.333+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T08:33:11.397+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T08:33:36.390+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T08:33:36.449+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T08:33:37.911+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T083308, end_date=20240603T083337
[2024-06-03T08:33:37.965+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T08:33:37.990+0000] {taskinstance.py:3498} INFO - 11 downstream tasks scheduled from follow-on schedule check
[2024-06-03T08:33:37.992+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T08:37:07.891+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T08:37:07.906+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:37:07.910+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:37:07.911+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T08:37:08.028+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T08:37:08.034+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=74) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T08:37:08.036+0000] {standard_task_runner.py:63} INFO - Started process 76 to run task
[2024-06-03T08:37:08.036+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmps2ds28zs']
[2024-06-03T08:37:08.038+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T08:37:08.079+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host f8718c862dab
[2024-06-03T08:37:08.138+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T08:37:08.138+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T08:37:11.044+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T08:37:36.493+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T08:37:36.563+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T08:37:38.063+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T083707, end_date=20240603T083738
[2024-06-03T08:37:38.125+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T08:37:38.151+0000] {taskinstance.py:3498} INFO - 11 downstream tasks scheduled from follow-on schedule check
[2024-06-03T08:37:38.153+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T08:40:59.043+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T08:40:59.059+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:40:59.064+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:40:59.064+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T08:40:59.173+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T08:40:59.177+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T08:40:59.178+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T08:40:59.179+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpy9sb7vwi']
[2024-06-03T08:40:59.180+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T08:40:59.218+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host b7f2aaa749e4
[2024-06-03T08:40:59.281+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T08:40:59.281+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T08:41:02.218+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T08:41:27.590+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T08:41:27.644+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T08:41:29.080+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T084059, end_date=20240603T084129
[2024-06-03T08:41:29.123+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T08:41:29.147+0000] {taskinstance.py:3498} INFO - 11 downstream tasks scheduled from follow-on schedule check
[2024-06-03T08:41:29.148+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T08:43:48.290+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T08:43:48.305+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:43:48.310+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:43:48.310+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T08:43:48.423+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T08:43:48.428+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T08:43:48.429+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T08:43:48.429+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpacd4e36i']
[2024-06-03T08:43:48.430+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T08:43:48.462+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 1b93f8e855cd
[2024-06-03T08:43:48.512+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T08:43:48.513+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T08:43:51.400+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T08:44:17.316+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T08:44:17.376+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T08:44:18.854+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T084348, end_date=20240603T084418
[2024-06-03T08:44:18.879+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T08:44:18.900+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T08:44:18.902+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T08:50:44.260+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T08:50:44.276+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:50:44.281+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:50:44.282+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T08:50:44.393+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T08:50:44.399+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpij3otr6a']
[2024-06-03T08:50:44.399+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=108) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T08:50:44.401+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T08:50:44.401+0000] {standard_task_runner.py:63} INFO - Started process 110 to run task
[2024-06-03T08:50:44.437+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 216c629358bb
[2024-06-03T08:50:44.487+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T08:50:44.488+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T08:50:47.380+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T08:51:12.859+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T08:51:12.920+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T08:51:14.366+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T085044, end_date=20240603T085114
[2024-06-03T08:51:14.428+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T08:51:14.450+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T08:51:14.452+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T08:56:18.514+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T08:56:18.529+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:56:18.534+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T08:56:18.535+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T08:56:18.640+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T08:56:18.644+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T08:56:18.645+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T08:56:18.645+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp8paqdn8t']
[2024-06-03T08:56:18.646+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T08:56:18.679+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host dc35e2debed5
[2024-06-03T08:56:18.730+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T08:56:18.731+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T08:56:21.568+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T08:56:47.163+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T08:56:47.215+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T08:56:48.651+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T085618, end_date=20240603T085648
[2024-06-03T08:56:48.721+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T08:56:48.744+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T08:56:48.746+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T09:05:06.998+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T09:05:07.012+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:05:07.016+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:05:07.016+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T09:05:07.111+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T09:05:07.115+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T09:05:07.116+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T09:05:07.116+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpv4whmgdv']
[2024-06-03T09:05:07.118+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T09:05:07.148+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host f68a8afe55d2
[2024-06-03T09:05:07.195+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T09:05:07.196+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T09:05:10.041+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T09:05:35.260+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T09:05:35.331+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T09:05:36.751+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T090507, end_date=20240603T090536
[2024-06-03T09:05:36.798+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T09:05:36.823+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T09:05:36.825+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T09:13:03.859+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T09:13:03.875+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:13:03.880+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:13:03.880+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T09:13:03.991+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T09:13:03.995+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=68) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T09:13:03.996+0000] {standard_task_runner.py:63} INFO - Started process 70 to run task
[2024-06-03T09:13:03.996+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpc2ekxbic']
[2024-06-03T09:13:03.997+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T09:13:04.040+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 4119bb8bb291
[2024-06-03T09:13:04.096+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T09:13:04.096+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T09:13:06.989+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T09:13:32.246+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T09:13:32.300+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T09:13:33.813+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T091303, end_date=20240603T091333
[2024-06-03T09:13:33.848+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T09:13:33.871+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T09:13:33.873+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T09:24:43.225+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T09:24:43.243+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:24:43.251+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:24:43.251+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T09:24:43.370+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T09:24:43.375+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=123) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T09:24:43.376+0000] {standard_task_runner.py:63} INFO - Started process 125 to run task
[2024-06-03T09:24:43.375+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp871tg631']
[2024-06-03T09:24:43.377+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T09:24:43.409+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 27c9a82b83bd
[2024-06-03T09:24:43.458+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T09:24:43.459+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T09:24:46.348+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T09:25:11.287+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T09:25:11.338+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T09:25:12.731+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T092443, end_date=20240603T092512
[2024-06-03T09:25:12.787+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T09:25:12.810+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T09:25:12.812+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T09:31:40.936+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T09:31:40.950+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:31:40.955+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:31:40.955+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T09:31:41.058+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T09:31:41.062+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T09:31:41.063+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T09:31:41.063+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp6a8761bh']
[2024-06-03T09:31:41.065+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T09:31:41.101+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host acc449a1db53
[2024-06-03T09:31:41.156+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T09:31:41.156+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T09:31:44.064+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T09:32:09.351+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T09:32:09.351+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3791, in get_loc
    return self._engine.get_loc(casted_key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "index.pyx", line 152, in pandas._libs.index.IndexEngine.get_loc
  File "index.pyx", line 181, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'fastestLaptime'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 265, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/etl_dag.py", line 52, in extract_data
    df['fastestLaptime'] = pd.to_timedelta(df['fastestLaptime'], 'seconds')
                                           ~~^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/frame.py", line 3893, in __getitem__
    indexer = self.columns.get_loc(key)
              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/indexes/base.py", line 3798, in get_loc
    raise KeyError(key) from err
KeyError: 'fastestLaptime'
[2024-06-03T09:32:09.360+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T093140, end_date=20240603T093209
[2024-06-03T09:32:09.367+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 3 for task extract_data ('fastestLaptime'; 69)
[2024-06-03T09:32:09.398+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-06-03T09:32:09.418+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-03T09:32:09.422+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T09:37:18.925+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T09:37:18.940+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:37:18.945+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:37:18.945+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T09:37:19.046+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T09:37:19.050+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=68) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T09:37:19.051+0000] {standard_task_runner.py:63} INFO - Started process 70 to run task
[2024-06-03T09:37:19.051+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmppnkzjtrg']
[2024-06-03T09:37:19.053+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T09:37:19.084+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 191d21591140
[2024-06-03T09:37:19.132+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T09:37:19.133+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T09:37:22.006+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T09:37:47.503+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T09:37:47.558+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T09:37:48.956+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T093718, end_date=20240603T093748
[2024-06-03T09:37:49.013+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T09:37:49.046+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T09:37:49.048+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T09:41:23.599+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T09:41:23.615+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:41:23.621+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:41:23.621+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T09:41:23.732+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T09:41:23.737+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T09:41:23.738+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T09:41:23.738+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp_ti3avol']
[2024-06-03T09:41:23.740+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T09:41:23.771+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host ac0500c9cdd5
[2024-06-03T09:41:23.821+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T09:41:23.822+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T09:41:26.820+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T09:41:52.014+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T09:41:52.065+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T09:41:53.501+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T094123, end_date=20240603T094153
[2024-06-03T09:41:53.561+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T09:41:53.586+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T09:41:53.589+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T09:44:11.863+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T09:44:11.879+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:44:11.885+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:44:11.885+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T09:44:11.985+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T09:44:11.989+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T09:44:11.990+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T09:44:11.990+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp8ipyxc4a']
[2024-06-03T09:44:11.991+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T09:44:12.023+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 4d4610b8dd99
[2024-06-03T09:44:12.072+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T09:44:12.072+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T09:44:14.995+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T09:44:40.042+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T09:44:40.095+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T09:44:41.543+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T094411, end_date=20240603T094441
[2024-06-03T09:44:41.567+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T09:44:41.574+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T09:48:37.984+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T09:48:38.000+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:48:38.005+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:48:38.005+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T09:48:38.114+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T09:48:38.119+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T09:48:38.120+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T09:48:38.119+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp32nhtz_0']
[2024-06-03T09:48:38.121+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T09:48:38.153+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 822b25182b4b
[2024-06-03T09:48:38.200+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T09:48:38.201+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T09:48:41.199+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T09:49:07.028+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T09:49:07.028+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py", line 265, in execute
    return_value = super().execute(context)
                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/etl_dag.py", line 45, in extract_data
    df[columns_to_numeric] = df[columns_to_numeric].apply(pd.to_numeric, errors='coerce', axis=1)
    ~~^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/frame.py", line 4079, in __setitem__
    self._setitem_array(key, value)
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/frame.py", line 4123, in _setitem_array
    self[k1] = value[k2]
    ~~~~^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/frame.py", line 4081, in __setitem__
    self._set_item_frame_value(key, value)
  File "/home/airflow/.local/lib/python3.12/site-packages/pandas/core/frame.py", line 4209, in _set_item_frame_value
    raise ValueError("Columns must be same length as key")
ValueError: Columns must be same length as key
[2024-06-03T09:49:07.037+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T094838, end_date=20240603T094907
[2024-06-03T09:49:07.045+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 3 for task extract_data (Columns must be same length as key; 62)
[2024-06-03T09:49:07.078+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-06-03T09:49:07.084+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T09:55:47.572+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T09:55:47.587+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:55:47.592+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:55:47.592+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T09:55:47.694+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T09:55:47.698+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T09:55:47.699+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T09:55:47.699+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmphswr3mni']
[2024-06-03T09:55:47.701+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T09:55:47.734+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 7d1472c6d87b
[2024-06-03T09:55:47.782+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T09:55:47.782+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T09:55:50.673+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T09:56:16.001+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T09:56:16.055+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T09:56:17.494+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T095547, end_date=20240603T095617
[2024-06-03T09:56:17.528+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T09:56:17.564+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T09:56:17.567+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T09:59:11.464+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T09:59:11.477+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:59:11.481+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T09:59:11.481+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T09:59:11.574+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T09:59:11.578+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=74) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T09:59:11.579+0000] {standard_task_runner.py:63} INFO - Started process 76 to run task
[2024-06-03T09:59:11.580+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp8fywcjje']
[2024-06-03T09:59:11.581+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T09:59:11.618+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 0c938bd99129
[2024-06-03T09:59:11.673+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T09:59:11.674+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T09:59:14.516+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T09:59:39.711+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T09:59:39.776+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T09:59:41.197+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T095911, end_date=20240603T095941
[2024-06-03T09:59:41.241+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T09:59:41.266+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T09:59:41.268+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T10:01:48.428+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T10:01:48.444+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:01:48.450+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:01:48.450+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T10:01:48.573+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T10:01:48.577+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T10:01:48.579+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T10:01:48.578+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp5s93s7p3']
[2024-06-03T10:01:48.580+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T10:01:48.614+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host a5fec9cf7dd1
[2024-06-03T10:01:48.721+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T10:01:48.722+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T10:01:51.670+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T10:02:16.896+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T10:02:16.949+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T10:02:18.393+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T100148, end_date=20240603T100218
[2024-06-03T10:02:18.433+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T10:02:18.456+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-03T10:02:18.461+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T10:05:08.235+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T10:05:08.251+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:05:08.256+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:05:08.257+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T10:05:08.359+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T10:05:08.363+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T10:05:08.364+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T10:05:08.364+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpza3id_z3']
[2024-06-03T10:05:08.366+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T10:05:08.398+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 4347a44b4f09
[2024-06-03T10:05:08.450+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T10:05:08.450+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T10:05:11.355+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T10:05:36.696+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T10:05:36.750+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T10:05:38.175+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T100508, end_date=20240603T100538
[2024-06-03T10:05:38.212+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T10:05:38.236+0000] {taskinstance.py:3498} INFO - 12 downstream tasks scheduled from follow-on schedule check
[2024-06-03T10:05:38.238+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T10:11:18.577+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T10:11:18.591+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:11:18.596+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:11:18.596+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T10:11:18.708+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T10:11:18.712+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=101) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T10:11:18.713+0000] {standard_task_runner.py:63} INFO - Started process 103 to run task
[2024-06-03T10:11:18.713+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpv3gf5kv4']
[2024-06-03T10:11:18.715+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T10:11:18.754+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 2949d36732ec
[2024-06-03T10:11:18.815+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T10:11:18.816+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T10:11:21.709+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T10:11:46.861+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T10:11:46.912+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T10:11:48.353+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T101118, end_date=20240603T101148
[2024-06-03T10:11:48.410+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T10:11:48.430+0000] {taskinstance.py:3498} INFO - 11 downstream tasks scheduled from follow-on schedule check
[2024-06-03T10:11:48.432+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T10:13:34.911+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T10:13:34.924+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:13:34.928+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:13:34.928+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T10:13:35.030+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T10:13:35.034+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T10:13:35.035+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T10:13:35.036+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpqwt0co5l']
[2024-06-03T10:13:35.038+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T10:13:35.075+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host bb971094f44b
[2024-06-03T10:13:35.133+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T10:13:35.133+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T10:13:38.031+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T10:14:03.393+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T10:14:03.447+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T10:14:04.888+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T101334, end_date=20240603T101404
[2024-06-03T10:14:04.932+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T10:14:04.955+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-03T10:14:04.956+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T10:20:36.943+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T10:20:36.957+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:20:36.962+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:20:36.963+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T10:20:37.064+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T10:20:37.068+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T10:20:37.069+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T10:20:37.069+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpzhgiwr3f']
[2024-06-03T10:20:37.071+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T10:20:37.102+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 0053659eca33
[2024-06-03T10:20:37.151+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T10:20:37.152+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T10:20:40.009+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T10:21:05.464+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T10:21:05.517+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T10:21:06.952+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T102036, end_date=20240603T102106
[2024-06-03T10:21:07.000+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T10:21:07.024+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T10:21:07.027+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T10:49:30.855+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T10:49:30.870+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:49:30.875+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T10:49:30.875+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T10:49:30.976+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T10:49:30.980+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T10:49:30.981+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T10:49:30.981+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmppud5z7z3']
[2024-06-03T10:49:30.982+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T10:49:31.014+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 1119030805b8
[2024-06-03T10:49:31.062+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T10:49:31.063+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T10:49:33.901+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T10:49:59.366+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T10:49:59.424+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T10:50:00.817+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T104930, end_date=20240603T105000
[2024-06-03T10:50:00.872+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T10:50:00.897+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T10:50:00.899+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T11:44:53.459+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T11:44:53.475+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T11:44:53.481+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T11:44:53.481+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T11:44:53.579+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T11:44:53.582+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T11:44:53.583+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T11:44:53.584+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp_btd_w5n']
[2024-06-03T11:44:53.585+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T11:44:53.616+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 72eb16cb1cc0
[2024-06-03T11:44:53.667+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T11:44:53.668+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T11:44:56.498+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T11:45:22.093+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T11:45:22.147+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T11:45:23.580+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T114453, end_date=20240603T114523
[2024-06-03T11:45:23.645+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T11:45:23.671+0000] {taskinstance.py:3498} INFO - 10 downstream tasks scheduled from follow-on schedule check
[2024-06-03T11:45:23.673+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T11:48:06.830+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T11:48:06.847+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T11:48:06.852+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T11:48:06.852+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T11:48:06.955+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T11:48:06.959+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T11:48:06.959+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T11:48:06.960+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmphupmxiyc']
[2024-06-03T11:48:06.961+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T11:48:06.995+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 87197cc50edf
[2024-06-03T11:48:07.045+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T11:48:07.046+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T11:48:09.922+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T11:48:35.650+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T11:48:35.702+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T11:48:37.139+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T114806, end_date=20240603T114837
[2024-06-03T11:48:37.181+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T11:48:37.205+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-03T11:48:37.208+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:01:39.848+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:01:39.865+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:01:39.870+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:01:39.870+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:01:39.992+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:01:39.997+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:01:39.998+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T12:01:39.998+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp6tgw1xka']
[2024-06-03T12:01:40.000+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:01:40.035+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 4a3738072664
[2024-06-03T12:01:40.089+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:01:40.090+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:01:42.906+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:02:08.785+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:02:08.845+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:02:10.285+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T120139, end_date=20240603T120210
[2024-06-03T12:02:10.328+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:02:10.350+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:02:10.352+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:05:04.006+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:05:04.028+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:05:04.033+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:05:04.033+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:05:04.144+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:05:04.149+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:05:04.150+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T12:05:04.150+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp_9oz06c6']
[2024-06-03T12:05:04.152+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:05:04.185+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 0b82a86b0d64
[2024-06-03T12:05:04.236+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:05:04.237+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:05:07.310+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:05:33.123+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:05:33.181+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:05:34.649+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T120504, end_date=20240603T120534
[2024-06-03T12:05:34.689+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:05:34.715+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:05:34.717+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:08:11.289+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:08:11.304+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:08:11.308+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:08:11.308+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:08:11.412+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:08:11.416+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:08:11.417+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T12:08:11.417+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp67th1c8c']
[2024-06-03T12:08:11.419+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:08:11.452+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 023837dae654
[2024-06-03T12:08:11.503+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:08:11.503+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:08:14.400+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:08:39.911+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:08:39.963+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:08:41.433+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T120811, end_date=20240603T120841
[2024-06-03T12:08:41.477+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:08:41.497+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:08:41.498+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:17:08.764+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:17:08.779+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:17:08.783+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:17:08.783+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:17:08.894+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:17:08.898+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:17:08.900+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T12:17:08.899+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpun8h9msc']
[2024-06-03T12:17:08.901+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:17:08.932+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 9dd224ba471c
[2024-06-03T12:17:08.981+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:17:08.982+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:17:11.869+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:17:37.640+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:17:37.693+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:17:39.117+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T121708, end_date=20240603T121739
[2024-06-03T12:17:39.165+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:17:39.188+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:17:39.189+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:19:44.866+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:19:44.881+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:19:44.886+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:19:44.886+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:19:44.987+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:19:44.991+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:19:44.992+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T12:19:44.992+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpt7ypk8ax']
[2024-06-03T12:19:44.994+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:19:45.025+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 1c521a2fd7c3
[2024-06-03T12:19:45.072+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:19:45.073+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:19:48.000+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:20:13.527+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:20:13.585+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:20:15.031+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T121944, end_date=20240603T122015
[2024-06-03T12:20:15.067+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:20:15.075+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:22:03.979+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:22:03.992+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:22:03.996+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:22:03.997+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:22:04.091+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:22:04.095+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:22:04.096+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T12:22:04.097+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp1xvebsk8']
[2024-06-03T12:22:04.098+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:22:04.131+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 96c3f57cfc51
[2024-06-03T12:22:04.182+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:22:04.183+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:22:07.023+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:22:32.883+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:22:32.936+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:22:34.352+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T122203, end_date=20240603T122234
[2024-06-03T12:22:34.383+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:22:34.405+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:22:34.406+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:25:04.760+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:25:04.775+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:25:04.779+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:25:04.779+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:25:04.887+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:25:04.891+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:25:04.892+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T12:25:04.892+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpr_zgmuh6']
[2024-06-03T12:25:04.894+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:25:04.929+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 309ef248c5d4
[2024-06-03T12:25:04.981+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:25:04.981+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:25:07.865+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:25:33.808+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:25:33.862+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:25:35.303+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T122504, end_date=20240603T122535
[2024-06-03T12:25:35.362+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:25:35.387+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:25:35.389+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:27:51.331+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:27:51.345+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:27:51.350+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:27:51.350+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:27:51.453+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:27:51.456+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=66) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:27:51.457+0000] {standard_task_runner.py:63} INFO - Started process 68 to run task
[2024-06-03T12:27:51.457+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpghd6zqq7']
[2024-06-03T12:27:51.459+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:27:51.491+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host f268f47424e3
[2024-06-03T12:27:51.544+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:27:51.545+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:27:54.465+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:28:20.301+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:28:20.364+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:28:21.821+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T122751, end_date=20240603T122821
[2024-06-03T12:28:21.881+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:28:21.905+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:28:21.906+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:33:28.952+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:33:28.967+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:33:28.971+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:33:28.971+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:33:29.072+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:33:29.075+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:33:29.077+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T12:33:29.077+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp6dni83yd']
[2024-06-03T12:33:29.078+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:33:29.112+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host acc64bd254b0
[2024-06-03T12:33:29.167+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:33:29.168+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:33:32.081+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:33:57.693+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:33:57.749+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:33:59.156+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T123328, end_date=20240603T123359
[2024-06-03T12:33:59.217+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:33:59.248+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:33:59.250+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:35:59.787+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:35:59.803+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:35:59.808+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:35:59.808+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:35:59.920+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:35:59.925+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:35:59.926+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T12:35:59.926+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp8yv3q364']
[2024-06-03T12:35:59.927+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:35:59.962+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host af9236f7a627
[2024-06-03T12:36:00.021+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:36:00.022+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:36:03.077+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:36:28.773+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:36:28.841+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:36:30.288+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T123559, end_date=20240603T123630
[2024-06-03T12:36:30.345+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:36:30.368+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:36:30.370+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:39:33.422+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:39:33.438+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:39:33.444+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:39:33.444+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:39:33.548+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:39:33.552+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=68) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:39:33.553+0000] {standard_task_runner.py:63} INFO - Started process 70 to run task
[2024-06-03T12:39:33.554+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpmgainxyb']
[2024-06-03T12:39:33.556+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:39:33.595+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 148bf491c0c1
[2024-06-03T12:39:33.657+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:39:33.658+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:39:36.538+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:40:02.272+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:40:02.325+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:40:03.754+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T123933, end_date=20240603T124003
[2024-06-03T12:40:03.794+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:40:03.817+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:40:03.818+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:46:50.628+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:46:50.643+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:46:50.647+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:46:50.647+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:46:50.750+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:46:50.754+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:46:50.755+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T12:46:50.755+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpzco3dmpw']
[2024-06-03T12:46:50.757+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:46:50.790+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 10f46dfb1cfb
[2024-06-03T12:46:50.843+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:46:50.844+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:46:53.763+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:47:20.447+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:47:20.507+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:47:22.043+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T124650, end_date=20240603T124722
[2024-06-03T12:47:22.092+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:47:22.113+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:47:22.114+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:49:16.044+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:49:16.058+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:49:16.063+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:49:16.063+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:49:16.167+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:49:16.171+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:49:16.172+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T12:49:16.172+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpg22sq1wc']
[2024-06-03T12:49:16.173+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:49:16.207+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host f87e90f5278c
[2024-06-03T12:49:16.259+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:49:16.260+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:49:19.115+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:49:44.863+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:49:44.918+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:49:46.378+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T124916, end_date=20240603T124946
[2024-06-03T12:49:46.431+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:49:46.464+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:49:46.466+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:51:46.870+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:51:46.883+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:51:46.888+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:51:46.889+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:51:47.016+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:51:47.019+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=61) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:51:47.020+0000] {standard_task_runner.py:63} INFO - Started process 63 to run task
[2024-06-03T12:51:47.021+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp0dbosxnc']
[2024-06-03T12:51:47.023+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:51:47.062+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 75def620a537
[2024-06-03T12:51:47.118+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:51:47.119+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:51:50.022+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:52:15.730+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:52:15.788+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:52:17.286+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T125146, end_date=20240603T125217
[2024-06-03T12:52:17.323+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:52:17.345+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:52:17.346+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T12:57:28.228+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T12:57:28.242+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:57:28.246+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T12:57:28.247+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T12:57:28.356+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T12:57:28.359+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T12:57:28.360+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T12:57:28.360+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpd4rcwl8m']
[2024-06-03T12:57:28.362+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T12:57:28.394+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host dd6d107969a3
[2024-06-03T12:57:28.443+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T12:57:28.443+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T12:57:31.252+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T12:57:56.522+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 72 columns]
[2024-06-03T12:57:56.589+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T12:57:58.113+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T125728, end_date=20240603T125758
[2024-06-03T12:57:58.168+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T12:57:58.197+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T12:57:58.198+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:05:41.704+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:05:41.721+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:05:41.727+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:05:41.727+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:05:41.835+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:05:41.840+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:05:41.841+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T13:05:41.840+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp9oluhi5t']
[2024-06-03T13:05:41.842+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:05:41.873+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host fdf5962c6ce8
[2024-06-03T13:05:41.921+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:05:41.922+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:05:44.856+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:06:10.148+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:06:10.203+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:06:11.614+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T130541, end_date=20240603T130611
[2024-06-03T13:06:11.672+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:06:11.696+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:06:11.698+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:09:45.274+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:09:45.290+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:09:45.294+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:09:45.295+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:09:45.404+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:09:45.411+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpve8jc9ct']
[2024-06-03T13:09:45.411+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:09:45.413+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:09:45.413+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T13:09:45.446+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host bc40b27929ff
[2024-06-03T13:09:45.496+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:09:45.497+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:09:48.401+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:10:13.872+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:10:13.944+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:10:15.346+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T130945, end_date=20240603T131015
[2024-06-03T13:10:15.389+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:10:15.411+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:10:15.413+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:13:58.902+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:13:58.917+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:13:58.923+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:13:58.923+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:13:59.026+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:13:59.030+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:13:59.031+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T13:13:59.031+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp8d795m5h']
[2024-06-03T13:13:59.032+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:13:59.065+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 079b70194df9
[2024-06-03T13:13:59.116+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:13:59.116+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:14:02.076+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:14:27.779+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:14:27.832+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:14:29.267+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T131358, end_date=20240603T131429
[2024-06-03T13:14:29.313+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:14:29.343+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:14:29.345+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:21:57.060+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:21:57.075+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:21:57.080+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:21:57.080+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:21:57.180+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:21:57.184+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:21:57.185+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T13:21:57.185+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpbn6jlgtw']
[2024-06-03T13:21:57.186+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:21:57.218+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 72b5fa973b1e
[2024-06-03T13:21:57.267+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:21:57.268+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:21:59.773+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:22:25.735+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:22:25.804+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:22:27.205+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T132157, end_date=20240603T132227
[2024-06-03T13:22:27.253+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:22:27.274+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:22:27.276+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:24:38.348+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:24:38.362+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:24:38.366+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:24:38.366+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:24:38.474+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:24:38.478+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:24:38.480+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T13:24:38.481+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpccbnp00h']
[2024-06-03T13:24:38.482+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:24:38.519+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 8a43854245bb
[2024-06-03T13:24:38.569+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:24:38.570+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:24:41.507+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:25:07.805+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:25:07.860+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:25:09.301+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T132438, end_date=20240603T132509
[2024-06-03T13:25:09.348+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:25:09.370+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:25:09.372+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:28:52.052+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:28:52.069+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:28:52.074+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:28:52.075+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:28:52.174+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:28:52.179+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:28:52.180+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T13:28:52.180+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpsvhygxk6']
[2024-06-03T13:28:52.182+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:28:52.214+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 820faf2ab561
[2024-06-03T13:28:52.263+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:28:52.264+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:28:55.347+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:29:20.941+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:29:20.994+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:29:22.424+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T132852, end_date=20240603T132922
[2024-06-03T13:29:22.478+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:29:22.501+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:29:22.503+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:31:30.580+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:31:30.596+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:31:30.602+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:31:30.602+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:31:30.710+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:31:30.714+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:31:30.715+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T13:31:30.715+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpwe3af8et']
[2024-06-03T13:31:30.716+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:31:30.749+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 358e05527583
[2024-06-03T13:31:30.798+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:31:30.799+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:31:33.707+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:31:59.620+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:31:59.678+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:32:01.150+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T133130, end_date=20240603T133201
[2024-06-03T13:32:01.213+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:32:01.237+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:32:01.238+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:34:25.966+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:34:25.980+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:34:25.985+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:34:25.985+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:34:26.117+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:34:26.122+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=68) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:34:26.123+0000] {standard_task_runner.py:63} INFO - Started process 70 to run task
[2024-06-03T13:34:26.123+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp46gk9aci']
[2024-06-03T13:34:26.125+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:34:26.161+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host e294eb76862f
[2024-06-03T13:34:26.212+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:34:26.213+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:34:29.135+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:34:55.107+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:34:55.160+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:34:56.588+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T133425, end_date=20240603T133456
[2024-06-03T13:34:56.639+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:34:56.662+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:34:56.663+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:37:52.661+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:37:52.677+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:37:52.682+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:37:52.682+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:37:52.787+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:37:52.791+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:37:52.792+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T13:37:52.792+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmprv8jzrgt']
[2024-06-03T13:37:52.794+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:37:52.827+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host c20b72d53e8b
[2024-06-03T13:37:52.878+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:37:52.879+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:37:55.769+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:38:20.796+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:38:20.849+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:38:22.259+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T133752, end_date=20240603T133822
[2024-06-03T13:38:22.287+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:38:22.311+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:38:22.313+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:40:30.506+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:40:30.521+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:40:30.526+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:40:30.526+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:40:30.626+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:40:30.630+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:40:30.631+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T13:40:30.631+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpvzjlsdar']
[2024-06-03T13:40:30.632+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:40:30.663+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 7b55653f5a2d
[2024-06-03T13:40:30.710+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:40:30.711+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:40:33.642+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:40:58.768+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:40:58.819+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:41:00.213+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T134030, end_date=20240603T134100
[2024-06-03T13:41:00.273+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:41:00.297+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:41:00.299+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T13:58:35.852+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T13:58:35.867+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:58:35.872+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T13:58:35.872+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T13:58:35.974+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T13:58:35.978+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T13:58:35.979+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T13:58:35.981+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpqsdwkzqj']
[2024-06-03T13:58:35.982+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T13:58:36.016+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host d95b8def734d
[2024-06-03T13:58:36.068+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T13:58:36.068+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T13:58:39.040+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T13:59:04.096+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T13:59:04.151+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T13:59:05.556+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T135835, end_date=20240603T135905
[2024-06-03T13:59:05.613+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T13:59:05.645+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T13:59:05.648+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:02:44.228+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:02:44.242+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:02:44.246+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:02:44.247+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:02:44.357+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:02:44.361+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=74) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:02:44.362+0000] {standard_task_runner.py:63} INFO - Started process 76 to run task
[2024-06-03T14:02:44.363+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmprca11lqs']
[2024-06-03T14:02:44.364+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:02:44.398+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host edc16a5e1b97
[2024-06-03T14:02:44.451+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:02:44.452+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:02:47.418+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:03:12.671+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:03:12.727+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:03:14.128+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T140244, end_date=20240603T140314
[2024-06-03T14:03:14.172+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:03:14.197+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:03:14.199+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:05:34.992+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:05:35.012+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:05:35.017+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:05:35.018+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:05:35.142+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:05:35.146+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:05:35.147+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T14:05:35.147+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpfva0uxo5']
[2024-06-03T14:05:35.149+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:05:35.181+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host a5361fc1545e
[2024-06-03T14:05:35.235+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:05:35.235+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:05:38.124+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:06:03.359+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:06:03.415+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:06:04.846+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T140535, end_date=20240603T140604
[2024-06-03T14:06:04.882+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:06:04.907+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:06:04.908+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:11:58.089+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:11:58.108+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:11:58.115+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:11:58.116+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:11:58.252+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:11:58.257+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:11:58.258+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T14:11:58.258+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpy29kbo8l']
[2024-06-03T14:11:58.260+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:11:58.293+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 57458d438a7a
[2024-06-03T14:11:58.345+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:11:58.346+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:12:01.328+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:12:26.525+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:12:26.588+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:12:27.998+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T141158, end_date=20240603T141227
[2024-06-03T14:12:28.041+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:12:28.066+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:12:28.069+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:15:11.815+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:15:11.829+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:15:11.835+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:15:11.836+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:15:11.947+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:15:11.952+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:15:11.953+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T14:15:11.953+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp8m0krx26']
[2024-06-03T14:15:11.954+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:15:11.986+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 22a6bb50d764
[2024-06-03T14:15:12.034+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:15:12.034+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:15:14.993+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:15:40.153+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:15:40.207+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:15:41.646+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T141511, end_date=20240603T141541
[2024-06-03T14:15:41.690+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:15:41.718+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:15:41.720+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:25:41.396+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:25:41.410+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:25:41.415+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:25:41.415+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:25:41.531+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:25:41.536+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:25:41.536+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T14:25:41.537+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpebie_z75']
[2024-06-03T14:25:41.538+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:25:41.576+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 0de6c9ac7879
[2024-06-03T14:25:41.638+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:25:41.639+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:25:44.579+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:26:09.801+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:26:09.853+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:26:11.277+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T142541, end_date=20240603T142611
[2024-06-03T14:26:11.338+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:26:11.363+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:26:11.365+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:30:33.305+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:30:33.320+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:30:33.324+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:30:33.325+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:30:33.439+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:30:33.444+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:30:33.445+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T14:30:33.445+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpurijzazl']
[2024-06-03T14:30:33.447+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:30:33.483+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host e215e198c4f4
[2024-06-03T14:30:33.544+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:30:33.545+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:30:36.485+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:31:01.612+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:31:01.664+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:31:03.088+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T143033, end_date=20240603T143103
[2024-06-03T14:31:03.126+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:31:03.148+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:31:03.150+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:33:09.401+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:33:09.415+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:33:09.419+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:33:09.419+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:33:09.529+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:33:09.534+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:33:09.535+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T14:33:09.535+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp5yrinqd1']
[2024-06-03T14:33:09.537+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:33:09.574+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host c6bc36c91dff
[2024-06-03T14:33:09.650+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:33:09.651+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:33:12.744+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:33:38.216+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:33:38.271+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:33:39.708+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T143309, end_date=20240603T143339
[2024-06-03T14:33:39.740+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:33:39.762+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:33:39.764+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:35:47.508+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:35:47.528+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:35:47.535+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:35:47.535+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:35:47.659+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:35:47.663+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:35:47.664+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T14:35:47.665+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpdf1vzatm']
[2024-06-03T14:35:47.666+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:35:47.705+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 13e1f5c75a30
[2024-06-03T14:35:47.769+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:35:47.769+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:35:50.790+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:36:15.840+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:36:15.893+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:36:17.299+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T143547, end_date=20240603T143617
[2024-06-03T14:36:17.326+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:36:17.353+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:36:17.354+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:41:54.247+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:41:54.261+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:41:54.265+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:41:54.266+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:41:54.442+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:41:54.448+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:41:54.449+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T14:41:54.449+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpzh07g3vi']
[2024-06-03T14:41:54.451+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:41:54.488+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 1eacd419231e
[2024-06-03T14:41:54.548+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:41:54.549+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:41:57.502+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:42:22.467+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:42:22.520+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:42:24.016+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T144154, end_date=20240603T144224
[2024-06-03T14:42:24.073+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:42:24.096+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:42:24.097+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:47:18.419+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:47:18.435+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:47:18.441+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:47:18.441+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:47:18.551+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:47:18.556+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:47:18.557+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-03T14:47:18.557+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp_40mbok5']
[2024-06-03T14:47:18.558+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:47:18.590+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host e08b3f871dde
[2024-06-03T14:47:18.638+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:47:18.639+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:47:21.586+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:47:46.584+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:47:46.636+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:47:48.041+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T144718, end_date=20240603T144748
[2024-06-03T14:47:48.077+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:47:48.098+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:47:48.100+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-03T14:52:34.897+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-03T14:52:34.915+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:52:34.921+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [queued]>
[2024-06-03T14:52:34.922+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-03T14:52:35.065+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-02 00:00:00+00:00
[2024-06-03T14:52:35.070+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-03T14:52:35.071+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-03T14:52:35.071+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-02T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpctjnhxxw']
[2024-06-03T14:52:35.072+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-03T14:52:35.115+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-02T00:00:00+00:00 [running]> on host 8631da0c0b16
[2024-06-03T14:52:35.171+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-02T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-02T00:00:00+00:00'
[2024-06-03T14:52:35.172+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-03T14:52:38.084+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-03T14:53:02.990+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-03T14:53:03.042+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-03T14:53:04.465+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-02T00:00:00+00:00, execution_date=20240602T000000, start_date=20240603T145234, end_date=20240603T145304
[2024-06-03T14:53:04.502+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-03T14:53:04.527+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-03T14:53:04.529+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
