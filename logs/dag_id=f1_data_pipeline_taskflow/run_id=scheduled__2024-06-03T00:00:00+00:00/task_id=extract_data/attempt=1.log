[2024-06-04T07:13:15.772+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T07:13:15.785+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T07:13:15.790+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T07:13:15.790+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T07:13:15.892+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T07:13:15.896+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=68) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T07:13:15.898+0000] {standard_task_runner.py:63} INFO - Started process 70 to run task
[2024-06-04T07:13:15.898+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp15gauype']
[2024-06-04T07:13:15.900+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T07:13:15.933+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host 66d2136786e1
[2024-06-04T07:13:15.984+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T07:13:15.984+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T07:13:18.864+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T07:13:43.328+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T07:13:43.382+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T07:13:44.774+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T071315, end_date=20240604T071344
[2024-06-04T07:13:44.804+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T07:13:44.825+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T07:13:44.826+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T07:47:13.303+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T07:47:13.318+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T07:47:13.323+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T07:47:13.323+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T07:47:13.436+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T07:47:13.442+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=88) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T07:47:13.443+0000] {standard_task_runner.py:63} INFO - Started process 90 to run task
[2024-06-04T07:47:13.443+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpknwv_i5y']
[2024-06-04T07:47:13.445+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T07:47:13.490+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host bbf1b45fa521
[2024-06-04T07:47:13.550+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T07:47:13.551+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T07:47:16.496+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T07:47:41.904+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T07:47:41.957+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T07:47:43.357+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T074713, end_date=20240604T074743
[2024-06-04T07:47:43.401+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T07:47:43.423+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T07:47:43.425+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T08:55:54.675+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T08:55:54.691+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T08:55:54.697+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T08:55:54.697+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T08:55:54.800+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T08:55:54.804+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=102) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T08:55:54.805+0000] {standard_task_runner.py:63} INFO - Started process 104 to run task
[2024-06-04T08:55:54.806+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpf8zwy31v']
[2024-06-04T08:55:54.807+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T08:55:54.843+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host 92ae07eb2785
[2024-06-04T08:55:54.904+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T08:55:54.905+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T08:55:57.799+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T08:56:23.690+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T08:56:23.743+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T08:56:25.163+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T085554, end_date=20240604T085625
[2024-06-04T08:56:25.199+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T08:56:25.220+0000] {taskinstance.py:3498} INFO - 13 downstream tasks scheduled from follow-on schedule check
[2024-06-04T08:56:25.221+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T08:59:55.379+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T08:59:55.394+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T08:59:55.398+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T08:59:55.398+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T08:59:55.622+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T08:59:55.626+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=60) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T08:59:55.627+0000] {standard_task_runner.py:63} INFO - Started process 62 to run task
[2024-06-04T08:59:55.627+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp0nbcw8wb']
[2024-06-04T08:59:55.629+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T08:59:55.675+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host 849b6d275aca
[2024-06-04T08:59:55.734+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T08:59:55.735+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T08:59:58.645+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T09:00:24.382+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T09:00:24.439+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T09:00:25.894+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T085955, end_date=20240604T090025
[2024-06-04T09:00:25.931+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T09:00:25.952+0000] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-06-04T09:00:25.953+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T09:14:06.226+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T09:14:06.240+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T09:14:06.245+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T09:14:06.245+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T09:14:06.350+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T09:14:06.355+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=81) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T09:14:06.356+0000] {standard_task_runner.py:63} INFO - Started process 83 to run task
[2024-06-04T09:14:06.356+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpdfdt5nex']
[2024-06-04T09:14:06.357+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T09:14:06.392+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host f99a8e5dd798
[2024-06-04T09:14:06.444+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T09:14:06.445+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T09:14:09.244+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T09:14:34.869+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T09:14:34.922+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T09:14:36.324+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T091406, end_date=20240604T091436
[2024-06-04T09:14:36.349+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T09:14:36.376+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T09:14:36.378+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T09:20:17.746+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T09:20:17.761+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T09:20:17.766+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T09:20:17.766+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T09:20:17.885+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T09:20:17.890+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T09:20:17.891+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-04T09:20:17.891+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp8kug7rqh']
[2024-06-04T09:20:17.893+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T09:20:17.941+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host 9a492cf60085
[2024-06-04T09:20:17.997+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T09:20:17.998+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T09:20:20.932+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:38: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T09:20:46.678+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T09:20:46.740+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T09:20:48.207+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T092017, end_date=20240604T092048
[2024-06-04T09:20:48.231+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T09:20:48.253+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T09:20:48.254+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T09:34:28.891+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T09:34:28.905+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T09:34:28.910+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T09:34:28.910+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T09:34:29.028+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T09:34:29.032+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=131) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T09:34:29.033+0000] {standard_task_runner.py:63} INFO - Started process 133 to run task
[2024-06-04T09:34:29.034+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpd5pb_igf']
[2024-06-04T09:34:29.036+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T09:34:29.069+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host 5ac195476051
[2024-06-04T09:34:29.130+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T09:34:29.130+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T09:34:32.079+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T09:34:57.306+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T09:34:57.357+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T09:34:58.782+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T093428, end_date=20240604T093458
[2024-06-04T09:34:58.811+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T09:34:58.834+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T09:34:58.836+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T10:21:19.112+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T10:21:19.126+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T10:21:19.131+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T10:21:19.131+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T10:21:19.239+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T10:21:19.243+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T10:21:19.244+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-04T10:21:19.244+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpxx3mempb']
[2024-06-04T10:21:19.245+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T10:21:19.278+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host 11fdb96bc2ba
[2024-06-04T10:21:19.328+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T10:21:19.329+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T10:21:22.224+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T10:21:48.042+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T10:21:48.113+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T10:21:49.511+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T102119, end_date=20240604T102149
[2024-06-04T10:21:49.546+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T10:21:49.567+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T10:21:49.568+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T12:37:31.151+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T12:37:31.167+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T12:37:31.172+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T12:37:31.172+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T12:37:31.278+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T12:37:31.282+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=152) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T12:37:31.283+0000] {standard_task_runner.py:63} INFO - Started process 154 to run task
[2024-06-04T12:37:31.283+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpi93iiiv3']
[2024-06-04T12:37:31.284+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T12:37:31.315+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host 7aeb235ee08d
[2024-06-04T12:37:31.366+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T12:37:31.366+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T12:37:34.400+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T12:37:59.355+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T12:37:59.425+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T12:38:00.868+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T123731, end_date=20240604T123800
[2024-06-04T12:38:00.931+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T12:38:00.953+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T12:38:00.954+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T13:04:37.004+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T13:04:37.020+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T13:04:37.026+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T13:04:37.027+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T13:04:37.161+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T13:04:37.165+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=74) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T13:04:37.166+0000] {standard_task_runner.py:63} INFO - Started process 76 to run task
[2024-06-04T13:04:37.166+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpbn8co2wr']
[2024-06-04T13:04:37.167+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T13:04:37.205+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host aaf296cae260
[2024-06-04T13:04:37.266+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T13:04:37.266+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T13:04:39.722+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T13:05:04.690+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T13:05:04.741+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T13:05:06.136+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T130437, end_date=20240604T130506
[2024-06-04T13:05:06.196+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T13:05:06.227+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T13:05:06.229+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T13:24:28.235+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T13:24:28.248+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T13:24:28.253+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T13:24:28.253+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T13:24:28.348+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T13:24:28.352+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T13:24:28.353+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-04T13:24:28.354+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp0t6lmp3e']
[2024-06-04T13:24:28.355+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T13:24:28.392+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host 55cd6db943bb
[2024-06-04T13:24:28.442+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T13:24:28.443+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T13:24:31.321+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T13:24:56.914+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T13:24:56.966+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T13:24:58.389+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T132428, end_date=20240604T132458
[2024-06-04T13:24:58.414+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T13:24:58.437+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T13:24:58.438+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T13:52:26.257+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T13:52:26.274+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T13:52:26.279+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T13:52:26.280+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T13:52:26.383+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T13:52:26.387+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T13:52:26.388+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-04T13:52:26.388+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpvi3dh3bn']
[2024-06-04T13:52:26.390+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T13:52:26.423+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host 43686a6526ed
[2024-06-04T13:52:26.475+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T13:52:26.476+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T13:52:29.354+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T13:52:54.512+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T13:52:54.570+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T13:52:56.062+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T135226, end_date=20240604T135256
[2024-06-04T13:52:56.115+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T13:52:56.139+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T13:52:56.141+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-04T14:00:59.891+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-04T14:00:59.906+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T14:00:59.911+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [queued]>
[2024-06-04T14:00:59.911+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-04T14:01:00.040+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-03 00:00:00+00:00
[2024-06-04T14:01:00.046+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=67) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-04T14:01:00.048+0000] {standard_task_runner.py:63} INFO - Started process 69 to run task
[2024-06-04T14:01:00.047+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-03T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmprk6qkqzh']
[2024-06-04T14:01:00.049+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-04T14:01:00.099+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-03T00:00:00+00:00 [running]> on host 7bdd51a4d37b
[2024-06-04T14:01:00.168+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-03T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-03T00:00:00+00:00'
[2024-06-04T14:01:00.169+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-04T14:01:03.133+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:40: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-04T14:01:28.057+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-04T14:01:28.115+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-04T14:01:29.603+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-03T00:00:00+00:00, execution_date=20240603T000000, start_date=20240604T140059, end_date=20240604T140129
[2024-06-04T14:01:29.660+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-04T14:01:29.685+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-04T14:01:29.687+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
