[2024-06-14T07:29:04.658+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T07:29:04.679+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T07:29:04.684+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T07:29:04.684+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T07:29:04.693+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T07:29:04.697+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=466) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T07:29:04.698+0000] {standard_task_runner.py:63} INFO - Started process 470 to run task
[2024-06-14T07:29:04.698+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpvdrde9ex']
[2024-06-14T07:29:04.700+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T07:29:04.857+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host aece75d0a06d
[2024-06-14T07:29:04.909+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T07:29:04.910+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T07:29:07.916+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T07:29:32.641+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T07:29:32.700+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T07:29:34.114+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T072904, end_date=20240614T072934
[2024-06-14T07:29:34.146+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T07:29:34.167+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T07:29:34.168+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T07:55:38.353+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T07:55:38.375+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T07:55:38.380+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T07:55:38.380+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T07:55:38.388+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T07:55:38.393+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T07:55:38.394+0000] {standard_task_runner.py:63} INFO - Started process 75 to run task
[2024-06-14T07:55:38.394+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpd86sd0g9']
[2024-06-14T07:55:38.396+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T07:55:38.557+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 22673327f9c1
[2024-06-14T07:55:38.608+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T07:55:38.608+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T07:55:41.133+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T07:56:06.848+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T07:56:06.908+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T07:56:08.323+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T075538, end_date=20240614T075608
[2024-06-14T07:56:08.372+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T07:56:08.394+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T07:56:08.396+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T08:02:31.092+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T08:02:31.120+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T08:02:31.129+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T08:02:31.130+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T08:02:31.147+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T08:02:31.155+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=76) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T08:02:31.157+0000] {standard_task_runner.py:63} INFO - Started process 85 to run task
[2024-06-14T08:02:31.157+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp_emrh0jl']
[2024-06-14T08:02:31.159+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T08:02:31.210+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 73204506bf77
[2024-06-14T08:02:31.291+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T08:02:31.292+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T08:02:34.251+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T08:02:59.386+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T08:02:59.445+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T08:03:00.968+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T080231, end_date=20240614T080300
[2024-06-14T08:03:01.014+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T08:03:01.037+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T08:03:01.039+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T08:04:27.461+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T08:04:27.482+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T08:04:27.487+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T08:04:27.487+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T08:04:27.496+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T08:04:27.500+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T08:04:27.501+0000] {standard_task_runner.py:63} INFO - Started process 73 to run task
[2024-06-14T08:04:27.501+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpi3lzjapk']
[2024-06-14T08:04:27.503+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T08:04:27.659+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 75ebf4a056df
[2024-06-14T08:04:27.711+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T08:04:27.712+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T08:04:30.611+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T08:04:55.267+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T08:04:55.331+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T08:04:56.801+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T080427, end_date=20240614T080456
[2024-06-14T08:04:56.836+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T08:04:56.858+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T08:04:56.860+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T08:32:53.406+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T08:32:53.429+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T08:32:53.433+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T08:32:53.433+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T08:32:53.442+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T08:32:53.447+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=85) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T08:32:53.448+0000] {standard_task_runner.py:63} INFO - Started process 87 to run task
[2024-06-14T08:32:53.448+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpb1doouzl']
[2024-06-14T08:32:53.449+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T08:32:53.606+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 02396e2d7ca5
[2024-06-14T08:32:53.660+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T08:32:53.661+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T08:32:56.582+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T08:33:22.158+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T08:33:22.217+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T08:33:23.622+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T083253, end_date=20240614T083323
[2024-06-14T08:33:23.667+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T08:33:23.689+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T08:33:23.692+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T08:55:56.798+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T08:55:56.822+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T08:55:56.828+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T08:55:56.828+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T08:55:56.837+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T08:55:56.842+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=162) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T08:55:56.842+0000] {standard_task_runner.py:63} INFO - Started process 164 to run task
[2024-06-14T08:55:56.843+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpmvveokcq']
[2024-06-14T08:55:56.845+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T08:55:57.004+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 78c0b56418d0
[2024-06-14T08:55:57.063+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T08:55:57.064+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T08:56:00.017+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T08:56:24.674+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T08:56:24.731+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T08:56:26.153+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T085556, end_date=20240614T085626
[2024-06-14T08:56:26.199+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T08:56:26.223+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T08:56:26.226+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T09:04:47.286+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T09:04:47.308+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T09:04:47.314+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T09:04:47.314+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T09:04:47.428+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T09:04:47.432+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=106) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T09:04:47.433+0000] {standard_task_runner.py:63} INFO - Started process 108 to run task
[2024-06-14T09:04:47.434+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp6gp69qq3']
[2024-06-14T09:04:47.435+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T09:04:47.475+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 2998375d03a6
[2024-06-14T09:04:47.534+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T09:04:47.534+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T09:04:50.427+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T09:05:15.182+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T09:05:15.243+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T09:05:16.644+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T090447, end_date=20240614T090516
[2024-06-14T09:05:16.688+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T09:05:16.710+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T09:05:16.712+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T09:26:49.781+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T09:26:49.803+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T09:26:49.810+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T09:26:49.810+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T09:26:49.821+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T09:26:49.825+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=99) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T09:26:49.826+0000] {standard_task_runner.py:63} INFO - Started process 101 to run task
[2024-06-14T09:26:49.827+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpigti6l_9']
[2024-06-14T09:26:49.828+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T09:26:49.974+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 05f8e2de7bcc
[2024-06-14T09:26:50.034+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T09:26:50.034+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T09:26:52.897+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T09:27:17.645+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T09:27:17.705+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T09:27:19.117+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T092649, end_date=20240614T092719
[2024-06-14T09:27:19.148+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T09:27:19.170+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T09:27:19.172+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T10:43:48.728+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T10:43:48.760+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T10:43:48.770+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T10:43:48.771+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T10:43:48.788+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T10:43:48.795+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T10:43:48.799+0000] {standard_task_runner.py:63} INFO - Started process 95 to run task
[2024-06-14T10:43:48.799+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpodhskh13']
[2024-06-14T10:43:48.802+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T10:43:49.257+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host d891683fa1de
[2024-06-14T10:43:49.351+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T10:43:49.352+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T10:43:52.474+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T10:44:18.733+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T10:44:18.810+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T10:46:35.157+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T10:46:35.171+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T10:46:35.175+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T10:46:35.175+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T10:46:35.183+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T10:46:35.187+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=78) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T10:46:35.188+0000] {standard_task_runner.py:63} INFO - Started process 83 to run task
[2024-06-14T10:46:35.189+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp6c6hvfxz']
[2024-06-14T10:46:35.190+0000] {standard_task_runner.py:91} INFO - Job 6: Subtask extract_data
[2024-06-14T10:46:35.224+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 3182b36e244b
[2024-06-14T10:46:35.276+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T10:46:35.277+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T10:46:37.892+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T10:47:03.208+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T10:47:03.265+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T10:47:04.671+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T104635, end_date=20240614T104704
[2024-06-14T10:47:04.732+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T10:47:04.756+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T10:47:04.757+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T10:51:21.045+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T10:51:21.075+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T10:51:21.082+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T10:51:21.083+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T10:51:21.357+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T10:51:21.363+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=81) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T10:51:21.365+0000] {standard_task_runner.py:63} INFO - Started process 94 to run task
[2024-06-14T10:51:21.366+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpco5p2pyg']
[2024-06-14T10:51:21.368+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T10:51:21.426+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host e391edf20c31
[2024-06-14T10:51:21.501+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T10:51:21.502+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T10:51:24.453+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T10:51:50.379+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T10:51:50.450+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T10:51:51.961+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T105121, end_date=20240614T105151
[2024-06-14T10:51:51.993+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T10:51:52.017+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T10:51:52.018+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T10:55:05.926+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T10:55:05.951+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T10:55:05.956+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T10:55:05.957+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T10:55:06.066+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T10:55:06.070+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=64) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T10:55:06.072+0000] {standard_task_runner.py:63} INFO - Started process 66 to run task
[2024-06-14T10:55:06.071+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpnwre0xhy']
[2024-06-14T10:55:06.073+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T10:55:06.108+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host c9e393690dc3
[2024-06-14T10:55:06.165+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T10:55:06.166+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T10:55:09.243+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T10:55:35.121+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T10:55:35.181+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T10:55:36.669+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T105505, end_date=20240614T105536
[2024-06-14T10:55:36.695+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T10:55:36.719+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T10:55:36.720+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T11:03:12.259+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T11:03:12.284+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:03:12.290+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:03:12.290+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T11:03:12.302+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T11:03:12.308+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=72) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T11:03:12.312+0000] {standard_task_runner.py:63} INFO - Started process 87 to run task
[2024-06-14T11:03:12.309+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpby3vmz5x']
[2024-06-14T11:03:12.312+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T11:03:12.384+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host cfca011b7e89
[2024-06-14T11:03:12.492+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T11:03:12.493+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T11:03:15.470+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T11:03:41.264+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T11:03:41.326+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T11:03:42.771+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T110312, end_date=20240614T110342
[2024-06-14T11:03:42.811+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T11:03:42.834+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T11:03:42.835+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T11:07:44.406+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T11:07:44.428+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:07:44.433+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:07:44.434+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T11:07:44.442+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T11:07:44.447+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=78) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T11:07:44.448+0000] {standard_task_runner.py:63} INFO - Started process 84 to run task
[2024-06-14T11:07:44.449+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp50nkeeng']
[2024-06-14T11:07:44.450+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T11:07:44.639+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 6e52e947d844
[2024-06-14T11:07:44.693+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T11:07:44.694+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T11:07:47.616+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T11:08:12.854+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T11:08:12.914+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T11:08:14.351+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T110744, end_date=20240614T110814
[2024-06-14T11:08:14.410+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T11:08:14.433+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T11:08:14.435+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T11:15:16.321+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T11:15:16.343+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:15:16.349+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:15:16.349+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T11:15:16.358+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T11:15:16.363+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=92) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T11:15:16.365+0000] {standard_task_runner.py:63} INFO - Started process 96 to run task
[2024-06-14T11:15:16.364+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp6qzbczg5']
[2024-06-14T11:15:16.366+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T11:15:16.539+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 82d779af27dd
[2024-06-14T11:15:16.595+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T11:15:16.596+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T11:15:19.530+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T11:15:44.839+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T11:15:44.906+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T11:15:46.433+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T111516, end_date=20240614T111546
[2024-06-14T11:15:46.495+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T11:15:46.517+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T11:15:46.520+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T11:20:37.921+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T11:20:37.945+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:20:37.952+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:20:37.952+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T11:20:37.964+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T11:20:37.968+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T11:20:37.970+0000] {standard_task_runner.py:63} INFO - Started process 73 to run task
[2024-06-14T11:20:37.969+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpmoofzq4r']
[2024-06-14T11:20:37.971+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T11:20:38.136+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 385858123cc9
[2024-06-14T11:20:38.185+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T11:20:38.186+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T11:20:41.059+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T11:21:06.544+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T11:21:06.602+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T11:21:08.015+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T112037, end_date=20240614T112108
[2024-06-14T11:21:08.056+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T11:21:08.078+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T11:21:08.080+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T11:25:39.382+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T11:25:39.407+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:25:39.414+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:25:39.415+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T11:25:39.424+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T11:25:39.428+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=92) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T11:25:39.429+0000] {standard_task_runner.py:63} INFO - Started process 94 to run task
[2024-06-14T11:25:39.429+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpuwjre5za']
[2024-06-14T11:25:39.431+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T11:25:39.594+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 3da481cb81cd
[2024-06-14T11:25:39.649+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T11:25:39.649+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T11:25:42.623+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T11:26:08.735+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T11:26:08.796+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T11:26:10.201+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T112539, end_date=20240614T112610
[2024-06-14T11:26:10.241+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T11:26:10.262+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T11:26:10.263+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T11:30:12.262+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T11:30:12.286+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:30:12.291+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:30:12.292+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T11:30:12.301+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T11:30:12.306+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=72) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T11:30:12.307+0000] {standard_task_runner.py:63} INFO - Started process 77 to run task
[2024-06-14T11:30:12.307+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpey2wsy4z']
[2024-06-14T11:30:12.309+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T11:30:12.503+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 397171faad63
[2024-06-14T11:30:12.563+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T11:30:12.564+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T11:30:15.596+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T11:30:41.123+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T11:30:41.195+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T11:30:42.708+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T113012, end_date=20240614T113042
[2024-06-14T11:30:42.736+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T11:30:42.762+0000] {taskinstance.py:3498} INFO - 9 downstream tasks scheduled from follow-on schedule check
[2024-06-14T11:30:42.764+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T11:34:53.646+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T11:34:53.668+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:34:53.673+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:34:53.673+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T11:34:53.682+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T11:34:53.687+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=72) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T11:34:53.688+0000] {standard_task_runner.py:63} INFO - Started process 77 to run task
[2024-06-14T11:34:53.688+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpcmrkr57r']
[2024-06-14T11:34:53.690+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T11:34:53.879+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host d0ca0375d865
[2024-06-14T11:34:53.936+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T11:34:53.936+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T11:34:56.896+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T11:35:22.197+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T11:35:22.268+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T11:35:23.701+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T113453, end_date=20240614T113523
[2024-06-14T11:35:23.740+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T11:35:23.762+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-14T11:35:23.763+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T11:42:14.653+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T11:42:14.677+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:42:14.682+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T11:42:14.682+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T11:42:14.692+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T11:42:14.697+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=78) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T11:42:14.698+0000] {standard_task_runner.py:63} INFO - Started process 82 to run task
[2024-06-14T11:42:14.699+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp6hl6v4jq']
[2024-06-14T11:42:14.701+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T11:42:14.889+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host aae9b98b8a12
[2024-06-14T11:42:14.951+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T11:42:14.952+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T11:42:18.008+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:39: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T11:42:43.056+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T11:42:43.116+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T11:42:44.524+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T114214, end_date=20240614T114244
[2024-06-14T11:42:44.576+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T11:42:44.598+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-14T11:42:44.600+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T12:16:46.130+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T12:16:46.153+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T12:16:46.158+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T12:16:46.158+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T12:16:46.268+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T12:16:46.272+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=71) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T12:16:46.273+0000] {standard_task_runner.py:63} INFO - Started process 73 to run task
[2024-06-14T12:16:46.275+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp2tsdkmtd']
[2024-06-14T12:16:46.277+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T12:16:46.313+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host e4ecbb11494f
[2024-06-14T12:16:46.366+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T12:16:46.367+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T12:16:49.285+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:37: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T12:17:14.932+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T12:17:15.007+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T12:17:16.417+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T121646, end_date=20240614T121716
[2024-06-14T12:17:16.482+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T12:17:16.521+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-14T12:17:16.523+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T12:44:17.401+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T12:44:17.422+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T12:44:17.426+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T12:44:17.427+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T12:44:17.536+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T12:44:17.541+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=70) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T12:44:17.542+0000] {standard_task_runner.py:63} INFO - Started process 72 to run task
[2024-06-14T12:44:17.542+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmp7yqkgm14']
[2024-06-14T12:44:17.544+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T12:44:17.579+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 23fced7d152c
[2024-06-14T12:44:17.633+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T12:44:17.634+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T12:44:20.489+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:37: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T12:44:46.282+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T12:44:46.346+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T12:44:47.814+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T124417, end_date=20240614T124447
[2024-06-14T12:44:47.843+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T12:44:47.864+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-14T12:44:47.865+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T13:31:50.861+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T13:31:50.884+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T13:31:50.888+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T13:31:50.888+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T13:31:51.011+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T13:31:51.016+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=64) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T13:31:51.017+0000] {standard_task_runner.py:63} INFO - Started process 66 to run task
[2024-06-14T13:31:51.017+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpaaw_q1qb']
[2024-06-14T13:31:51.018+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T13:31:51.054+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 5a14b9f42b04
[2024-06-14T13:31:51.111+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T13:31:51.112+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T13:31:54.124+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:37: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T13:32:19.655+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T13:32:19.712+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T13:32:21.167+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T133150, end_date=20240614T133221
[2024-06-14T13:32:21.227+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T13:32:21.251+0000] {taskinstance.py:3498} INFO - 8 downstream tasks scheduled from follow-on schedule check
[2024-06-14T13:32:21.253+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-14T13:56:04.802+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-14T13:56:04.826+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T13:56:04.831+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [queued]>
[2024-06-14T13:56:04.832+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-14T13:56:04.840+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): extract_data> on 2024-06-13 00:00:00+00:00
[2024-06-14T13:56:04.845+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=92) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-14T13:56:04.846+0000] {standard_task_runner.py:63} INFO - Started process 94 to run task
[2024-06-14T13:56:04.846+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_taskflow', 'extract_data', 'scheduled__2024-06-13T00:00:00+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpogqp08zd']
[2024-06-14T13:56:04.847+0000] {standard_task_runner.py:91} INFO - Job 3: Subtask extract_data
[2024-06-14T13:56:04.986+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_taskflow.extract_data scheduled__2024-06-13T00:00:00+00:00 [running]> on host 32b182b24782
[2024-06-14T13:56:05.037+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_taskflow' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2024-06-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-13T00:00:00+00:00'
[2024-06-14T13:56:05.037+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-14T13:56:07.941+0000] {warnings.py:110} WARNING - /opt/***/dags/etl_dag.py:37: DtypeWarning: Columns (7,8,13,14,17,42,45,63) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(csv_path)

[2024-06-14T13:56:33.203+0000] {python.py:237} INFO - Done. Returned value was:         Unnamed: 0  resultId  ...  wins_constructorstandings    status
0                0     21232  ...                          1  Finished
1                1     21232  ...                          1  Finished
2                2     21232  ...                          1  Finished
3                3     21232  ...                          1  Finished
4                4     21232  ...                          1  Finished
...            ...       ...  ...                        ...       ...
518412      518412     23041  ...                          0   +4 Laps
518413      518413     23041  ...                          0   +4 Laps
518414      518414     23041  ...                          0   +4 Laps
518415      518415     23041  ...                          0   +4 Laps
518416      518416     23041  ...                          0   +4 Laps

[518417 rows x 71 columns]
[2024-06-14T13:56:33.271+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-14T13:56:34.739+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_taskflow, task_id=extract_data, run_id=scheduled__2024-06-13T00:00:00+00:00, execution_date=20240613T000000, start_date=20240614T135604, end_date=20240614T135634
[2024-06-14T13:56:34.792+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-14T13:56:34.819+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-14T13:56:34.842+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
