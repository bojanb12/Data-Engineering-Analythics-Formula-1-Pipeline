[2024-06-19T07:29:12.410+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T07:29:12.442+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T07:29:12.457+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T07:29:12.458+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T07:29:12.473+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T07:29:12.479+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=638) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T07:29:12.480+0000] {standard_task_runner.py:63} INFO - Started process 657 to run task
[2024-06-19T07:29:12.481+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '37', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmpow9u7ol6']
[2024-06-19T07:29:12.483+0000] {standard_task_runner.py:91} INFO - Job 37: Subtask fetch_and_publish_constructor
[2024-06-19T07:29:12.527+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host 5d5bc16a80d8
[2024-06-19T07:29:12.619+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T07:29:12.620+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T07:29:15.532+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T07:29:15.533+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.18.0.6', 9092)]>: connecting to kafka:9092 [('172.18.0.6', 9092) IPv4]
[2024-06-19T07:29:15.533+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T07:29:15.534+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.18.0.6', 9092)]>: Connection complete.
[2024-06-19T07:29:15.636+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T07:29:15.636+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T07:29:15.790+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.18.0.6', 9092)]>: connecting to kafka:9092 [('172.18.0.6', 9092) IPv4]
[2024-06-19T07:29:15.790+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.18.0.6', 9092)]>: Connection complete.
[2024-06-19T07:29:15.790+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.18.0.6', 9092)]>: Closing connection. 
[2024-06-19T07:29:15.901+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T07:29:15.905+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.18.0.6', 9092)]>: Closing connection. 
[2024-06-19T07:29:15.906+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T07:29:15.906+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T07:29:15.907+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T07:29:15.916+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T072912, end_date=20240619T072915
[2024-06-19T07:29:15.943+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T07:29:15.955+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T07:29:15.956+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T08:27:34.864+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T08:27:34.886+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T08:27:34.895+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T08:27:34.895+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T08:27:34.905+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T08:27:34.913+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=607) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T08:27:34.914+0000] {standard_task_runner.py:63} INFO - Started process 627 to run task
[2024-06-19T08:27:34.914+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '35', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmp33nrtnpv']
[2024-06-19T08:27:34.916+0000] {standard_task_runner.py:91} INFO - Job 35: Subtask fetch_and_publish_constructor
[2024-06-19T08:27:34.957+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host d64590577fb7
[2024-06-19T08:27:35.030+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T08:27:35.031+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T08:27:37.238+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T08:27:37.240+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.19.0.6', 9092)]>: connecting to kafka:9092 [('172.19.0.6', 9092) IPv4]
[2024-06-19T08:27:37.240+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T08:27:37.241+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.19.0.6', 9092)]>: Connection complete.
[2024-06-19T08:27:37.344+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T08:27:37.344+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T08:27:37.500+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.19.0.6', 9092)]>: connecting to kafka:9092 [('172.19.0.6', 9092) IPv4]
[2024-06-19T08:27:37.501+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.19.0.6', 9092)]>: Connection complete.
[2024-06-19T08:27:37.501+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.19.0.6', 9092)]>: Closing connection. 
[2024-06-19T08:27:37.614+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T08:27:37.618+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.19.0.6', 9092)]>: Closing connection. 
[2024-06-19T08:27:37.618+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T08:27:37.619+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T08:27:37.619+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T08:27:37.625+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T082734, end_date=20240619T082737
[2024-06-19T08:27:37.656+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T08:27:37.666+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T08:27:37.668+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T08:54:42.115+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T08:54:42.133+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T08:54:42.140+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T08:54:42.140+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T08:54:42.148+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T08:54:42.153+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=610) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T08:54:42.154+0000] {standard_task_runner.py:63} INFO - Started process 634 to run task
[2024-06-19T08:54:42.154+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '42', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmptbt_0iyv']
[2024-06-19T08:54:42.156+0000] {standard_task_runner.py:91} INFO - Job 42: Subtask fetch_and_publish_constructor
[2024-06-19T08:54:42.188+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host e5c8778eab06
[2024-06-19T08:54:42.242+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T08:54:42.243+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T08:54:46.369+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T08:54:46.371+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.20.0.6', 9092)]>: connecting to kafka:9092 [('172.20.0.6', 9092) IPv4]
[2024-06-19T08:54:46.371+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T08:54:46.372+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.20.0.6', 9092)]>: Connection complete.
[2024-06-19T08:54:46.474+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T08:54:46.474+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T08:54:46.628+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.20.0.6', 9092)]>: connecting to kafka:9092 [('172.20.0.6', 9092) IPv4]
[2024-06-19T08:54:46.628+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.20.0.6', 9092)]>: Connection complete.
[2024-06-19T08:54:46.629+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.20.0.6', 9092)]>: Closing connection. 
[2024-06-19T08:54:46.742+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T08:54:46.746+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.20.0.6', 9092)]>: Closing connection. 
[2024-06-19T08:54:46.746+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T08:54:46.747+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T08:54:46.747+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T08:54:46.754+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T085442, end_date=20240619T085446
[2024-06-19T08:54:46.783+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T08:54:46.793+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T08:54:46.794+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T12:05:57.510+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T12:05:57.532+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:05:57.541+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:05:57.541+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T12:05:57.552+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T12:05:57.558+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=611) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T12:05:57.559+0000] {standard_task_runner.py:63} INFO - Started process 628 to run task
[2024-06-19T12:05:57.560+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '35', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmpg7_jyt_b']
[2024-06-19T12:05:57.562+0000] {standard_task_runner.py:91} INFO - Job 35: Subtask fetch_and_publish_constructor
[2024-06-19T12:05:57.607+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host eef124934bf8
[2024-06-19T12:05:57.685+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T12:05:57.687+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T12:05:59.521+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T12:05:59.522+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.21.0.6', 9092)]>: connecting to kafka:9092 [('172.21.0.6', 9092) IPv4]
[2024-06-19T12:05:59.523+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T12:05:59.524+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.21.0.6', 9092)]>: Connection complete.
[2024-06-19T12:05:59.627+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T12:05:59.627+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T12:05:59.782+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.21.0.6', 9092)]>: connecting to kafka:9092 [('172.21.0.6', 9092) IPv4]
[2024-06-19T12:05:59.782+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.21.0.6', 9092)]>: Connection complete.
[2024-06-19T12:05:59.782+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.21.0.6', 9092)]>: Closing connection. 
[2024-06-19T12:05:59.895+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T12:05:59.900+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.21.0.6', 9092)]>: Closing connection. 
[2024-06-19T12:05:59.901+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T12:05:59.901+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T12:05:59.902+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T12:05:59.910+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T120557, end_date=20240619T120559
[2024-06-19T12:05:59.938+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T12:05:59.951+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T12:05:59.952+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T12:37:09.464+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T12:37:09.514+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:37:09.532+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:37:09.532+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T12:37:09.551+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T12:37:09.562+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=647) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T12:37:09.565+0000] {standard_task_runner.py:63} INFO - Started process 676 to run task
[2024-06-19T12:37:09.565+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '39', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmp4_ncukb6']
[2024-06-19T12:37:09.568+0000] {standard_task_runner.py:91} INFO - Job 39: Subtask fetch_and_publish_constructor
[2024-06-19T12:37:09.633+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host 3a98e934d90a
[2024-06-19T12:37:09.738+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T12:37:09.739+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T12:37:13.448+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T12:37:13.450+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.22.0.6', 9092)]>: connecting to kafka:9092 [('172.22.0.6', 9092) IPv4]
[2024-06-19T12:37:13.450+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T12:37:13.450+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.22.0.6', 9092)]>: Connection complete.
[2024-06-19T12:37:13.553+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T12:37:13.553+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T12:37:13.707+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.22.0.6', 9092)]>: connecting to kafka:9092 [('172.22.0.6', 9092) IPv4]
[2024-06-19T12:37:13.708+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.22.0.6', 9092)]>: Connection complete.
[2024-06-19T12:37:13.708+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.22.0.6', 9092)]>: Closing connection. 
[2024-06-19T12:37:13.823+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T12:37:13.829+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.22.0.6', 9092)]>: Closing connection. 
[2024-06-19T12:37:13.830+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T12:37:13.831+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T12:37:13.831+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T12:37:13.839+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T123709, end_date=20240619T123713
[2024-06-19T12:37:13.874+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T12:37:13.886+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T12:37:13.887+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T12:53:40.201+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T12:53:40.219+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:53:40.225+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T12:53:40.226+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T12:53:40.233+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T12:53:40.239+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=628) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T12:53:40.239+0000] {standard_task_runner.py:63} INFO - Started process 646 to run task
[2024-06-19T12:53:40.240+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '42', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmp7ejx_r05']
[2024-06-19T12:53:40.241+0000] {standard_task_runner.py:91} INFO - Job 42: Subtask fetch_and_publish_constructor
[2024-06-19T12:53:40.272+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host 9426bfe013d2
[2024-06-19T12:53:40.328+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T12:53:40.329+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T12:53:42.039+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T12:53:42.041+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.23.0.6', 9092)]>: connecting to kafka:9092 [('172.23.0.6', 9092) IPv4]
[2024-06-19T12:53:42.041+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T12:53:42.042+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.23.0.6', 9092)]>: Connection complete.
[2024-06-19T12:53:42.144+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T12:53:42.144+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T12:53:42.298+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.23.0.6', 9092)]>: connecting to kafka:9092 [('172.23.0.6', 9092) IPv4]
[2024-06-19T12:53:42.298+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.23.0.6', 9092)]>: Connection complete.
[2024-06-19T12:53:42.298+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.23.0.6', 9092)]>: Closing connection. 
[2024-06-19T12:53:42.410+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T12:53:42.414+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.23.0.6', 9092)]>: Closing connection. 
[2024-06-19T12:53:42.415+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T12:53:42.415+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T12:53:42.416+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T12:53:42.423+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T125340, end_date=20240619T125342
[2024-06-19T12:53:42.460+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T12:53:42.471+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T12:53:42.472+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:03:39.801+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:03:39.831+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:03:39.840+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:03:39.841+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T13:03:39.856+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:03:39.862+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=606) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T13:03:39.864+0000] {standard_task_runner.py:63} INFO - Started process 621 to run task
[2024-06-19T13:03:39.864+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '34', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmpzp47jeka']
[2024-06-19T13:03:39.867+0000] {standard_task_runner.py:91} INFO - Job 34: Subtask fetch_and_publish_constructor
[2024-06-19T13:03:39.916+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host 6fa64189ef34
[2024-06-19T13:03:39.993+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:03:39.994+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:03:42.062+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T13:03:42.064+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.24.0.6', 9092)]>: connecting to kafka:9092 [('172.24.0.6', 9092) IPv4]
[2024-06-19T13:03:42.064+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T13:03:42.065+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.24.0.6', 9092)]>: Connection complete.
[2024-06-19T13:03:42.192+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T13:03:42.192+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T13:03:42.347+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.24.0.6', 9092)]>: connecting to kafka:9092 [('172.24.0.6', 9092) IPv4]
[2024-06-19T13:03:42.347+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.24.0.6', 9092)]>: Connection complete.
[2024-06-19T13:03:42.347+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.24.0.6', 9092)]>: Closing connection. 
[2024-06-19T13:03:42.807+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T13:03:42.829+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.24.0.6', 9092)]>: Closing connection. 
[2024-06-19T13:03:42.829+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T13:03:42.830+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T13:03:42.830+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:03:42.838+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T130339, end_date=20240619T130342
[2024-06-19T13:03:42.886+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:03:42.899+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:03:42.901+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:17:26.301+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:17:26.322+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:17:26.328+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:17:26.328+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T13:17:26.338+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:17:26.344+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=611) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T13:17:26.345+0000] {standard_task_runner.py:63} INFO - Started process 634 to run task
[2024-06-19T13:17:26.346+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '41', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmpf5iimlu1']
[2024-06-19T13:17:26.348+0000] {standard_task_runner.py:91} INFO - Job 41: Subtask fetch_and_publish_constructor
[2024-06-19T13:17:26.389+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host d5b44298ec9f
[2024-06-19T13:17:26.454+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:17:26.455+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:17:26.544+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T13:17:26.546+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: connecting to kafka:9092 [('172.25.0.6', 9092) IPv4]
[2024-06-19T13:17:26.547+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T13:17:26.547+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: Connection complete.
[2024-06-19T13:17:26.650+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T13:17:26.651+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T13:17:26.805+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: connecting to kafka:9092 [('172.25.0.6', 9092) IPv4]
[2024-06-19T13:17:26.805+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.25.0.6', 9092)]>: Connection complete.
[2024-06-19T13:17:26.806+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.25.0.6', 9092)]>: Closing connection. 
[2024-06-19T13:17:26.920+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T13:17:26.923+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.25.0.6', 9092)]>: Closing connection. 
[2024-06-19T13:17:26.924+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T13:17:26.924+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T13:17:26.925+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:17:26.933+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T131726, end_date=20240619T131726
[2024-06-19T13:17:26.960+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:17:26.973+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:17:26.974+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:27:56.211+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:27:56.229+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:27:56.235+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:27:56.235+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T13:27:56.244+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:27:56.249+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=618) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T13:27:56.250+0000] {standard_task_runner.py:63} INFO - Started process 640 to run task
[2024-06-19T13:27:56.251+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '42', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmp6tkoh7lu']
[2024-06-19T13:27:56.252+0000] {standard_task_runner.py:91} INFO - Job 42: Subtask fetch_and_publish_constructor
[2024-06-19T13:27:56.287+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host fc0cf0aa1225
[2024-06-19T13:27:56.345+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:27:56.346+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:27:56.435+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T13:27:56.436+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.26.0.6', 9092)]>: connecting to kafka:9092 [('172.26.0.6', 9092) IPv4]
[2024-06-19T13:27:56.436+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T13:27:56.437+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.26.0.6', 9092)]>: Connection complete.
[2024-06-19T13:27:56.541+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T13:27:56.541+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T13:27:56.694+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.26.0.6', 9092)]>: connecting to kafka:9092 [('172.26.0.6', 9092) IPv4]
[2024-06-19T13:27:56.695+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.26.0.6', 9092)]>: Connection complete.
[2024-06-19T13:27:56.695+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.26.0.6', 9092)]>: Closing connection. 
[2024-06-19T13:27:56.808+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T13:27:56.811+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.26.0.6', 9092)]>: Closing connection. 
[2024-06-19T13:27:56.812+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T13:27:56.812+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T13:27:56.812+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:27:56.819+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T132756, end_date=20240619T132756
[2024-06-19T13:27:56.865+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:27:56.876+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:27:56.877+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T13:38:03.254+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T13:38:03.280+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:38:03.289+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T13:38:03.289+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T13:38:03.302+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T13:38:03.308+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=594) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T13:38:03.309+0000] {standard_task_runner.py:63} INFO - Started process 619 to run task
[2024-06-19T13:38:03.310+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '39', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmpbf3yk088']
[2024-06-19T13:38:03.312+0000] {standard_task_runner.py:91} INFO - Job 39: Subtask fetch_and_publish_constructor
[2024-06-19T13:38:03.356+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host 790bccf90551
[2024-06-19T13:38:03.418+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T13:38:03.419+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T13:38:03.510+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T13:38:03.511+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.29.0.6', 9092)]>: connecting to kafka:9092 [('172.29.0.6', 9092) IPv4]
[2024-06-19T13:38:03.511+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T13:38:03.512+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.29.0.6', 9092)]>: Connection complete.
[2024-06-19T13:38:03.748+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T13:38:03.748+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T13:38:03.922+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.29.0.6', 9092)]>: connecting to kafka:9092 [('172.29.0.6', 9092) IPv4]
[2024-06-19T13:38:03.922+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.29.0.6', 9092)]>: Connection complete.
[2024-06-19T13:38:03.922+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.29.0.6', 9092)]>: Closing connection. 
[2024-06-19T13:38:04.389+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T13:38:04.479+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.29.0.6', 9092)]>: Closing connection. 
[2024-06-19T13:38:04.480+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T13:38:04.480+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T13:38:04.480+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T13:38:04.488+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T133803, end_date=20240619T133804
[2024-06-19T13:38:04.526+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T13:38:04.538+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T13:38:04.540+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T14:01:47.365+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T14:01:47.394+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:01:47.404+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:01:47.404+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T14:01:47.415+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T14:01:47.421+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=603) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T14:01:47.422+0000] {standard_task_runner.py:63} INFO - Started process 629 to run task
[2024-06-19T14:01:47.423+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '41', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmpqt7bq5wv']
[2024-06-19T14:01:47.425+0000] {standard_task_runner.py:91} INFO - Job 41: Subtask fetch_and_publish_constructor
[2024-06-19T14:01:47.466+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host 0ff0d3189758
[2024-06-19T14:01:47.525+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T14:01:47.526+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T14:01:47.620+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T14:01:47.622+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.30.0.6', 9092)]>: connecting to kafka:9092 [('172.30.0.6', 9092) IPv4]
[2024-06-19T14:01:47.622+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T14:01:47.623+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.30.0.6', 9092)]>: Connection complete.
[2024-06-19T14:01:47.849+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T14:01:47.849+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T14:01:48.003+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.30.0.6', 9092)]>: connecting to kafka:9092 [('172.30.0.6', 9092) IPv4]
[2024-06-19T14:01:48.004+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.30.0.6', 9092)]>: Connection complete.
[2024-06-19T14:01:48.004+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.30.0.6', 9092)]>: Closing connection. 
[2024-06-19T14:01:48.636+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T14:01:48.740+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.30.0.6', 9092)]>: Closing connection. 
[2024-06-19T14:01:48.741+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T14:01:48.742+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T14:01:48.742+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T14:01:48.751+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T140147, end_date=20240619T140148
[2024-06-19T14:01:48.799+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T14:01:48.806+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T14:05:24.932+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T14:05:24.958+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:05:24.966+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:05:24.967+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T14:05:24.978+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T14:05:24.984+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=597) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T14:05:24.985+0000] {standard_task_runner.py:63} INFO - Started process 614 to run task
[2024-06-19T14:05:24.986+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '33', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmpppfeodbo']
[2024-06-19T14:05:24.988+0000] {standard_task_runner.py:91} INFO - Job 33: Subtask fetch_and_publish_constructor
[2024-06-19T14:05:25.032+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host dc92dd71b4a2
[2024-06-19T14:05:25.104+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T14:05:25.106+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T14:05:26.494+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T14:05:26.496+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.31.0.6', 9092)]>: connecting to kafka:9092 [('172.31.0.6', 9092) IPv4]
[2024-06-19T14:05:26.496+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T14:05:26.497+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('172.31.0.6', 9092)]>: Connection complete.
[2024-06-19T14:05:26.603+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T14:05:26.603+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T14:05:26.758+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.31.0.6', 9092)]>: connecting to kafka:9092 [('172.31.0.6', 9092) IPv4]
[2024-06-19T14:05:26.759+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('172.31.0.6', 9092)]>: Connection complete.
[2024-06-19T14:05:26.759+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('172.31.0.6', 9092)]>: Closing connection. 
[2024-06-19T14:05:26.892+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T14:05:26.895+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('172.31.0.6', 9092)]>: Closing connection. 
[2024-06-19T14:05:26.896+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T14:05:26.897+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T14:05:26.897+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T14:05:26.904+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T140524, end_date=20240619T140526
[2024-06-19T14:05:26.923+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T14:05:26.935+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T14:05:26.936+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T14:14:26.242+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T14:14:26.263+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:14:26.270+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:14:26.270+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T14:14:26.278+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T14:14:26.284+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=635) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T14:14:26.285+0000] {standard_task_runner.py:63} INFO - Started process 659 to run task
[2024-06-19T14:14:26.286+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '40', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmp8quw3qu1']
[2024-06-19T14:14:26.287+0000] {standard_task_runner.py:91} INFO - Job 40: Subtask fetch_and_publish_constructor
[2024-06-19T14:14:26.323+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host 2acd74b25e37
[2024-06-19T14:14:26.386+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T14:14:26.387+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T14:14:28.253+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T14:14:28.254+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('192.168.0.6', 9092)]>: connecting to kafka:9092 [('192.168.0.6', 9092) IPv4]
[2024-06-19T14:14:28.255+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T14:14:28.255+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('192.168.0.6', 9092)]>: Connection complete.
[2024-06-19T14:14:28.358+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T14:14:28.358+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T14:14:28.512+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('192.168.0.6', 9092)]>: connecting to kafka:9092 [('192.168.0.6', 9092) IPv4]
[2024-06-19T14:14:28.512+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('192.168.0.6', 9092)]>: Connection complete.
[2024-06-19T14:14:28.512+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('192.168.0.6', 9092)]>: Closing connection. 
[2024-06-19T14:14:28.623+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T14:14:28.628+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('192.168.0.6', 9092)]>: Closing connection. 
[2024-06-19T14:14:28.628+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T14:14:28.629+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T14:14:28.629+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T14:14:28.637+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T141426, end_date=20240619T141428
[2024-06-19T14:14:28.665+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T14:14:28.677+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T14:14:28.678+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
[2024-06-19T14:26:23.244+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-06-19T14:26:23.262+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:26:23.269+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [queued]>
[2024-06-19T14:26:23.269+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-06-19T14:26:23.278+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): fetch_and_publish_constructor> on 2024-06-18 00:00:00+00:00
[2024-06-19T14:26:23.284+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=617) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2024-06-19T14:26:23.284+0000] {standard_task_runner.py:63} INFO - Started process 640 to run task
[2024-06-19T14:26:23.285+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'f1_data_pipeline_kafka_fetchAndPublish', 'fetch_and_publish_constructor', 'scheduled__2024-06-18T00:00:00+00:00', '--job-id', '42', '--raw', '--subdir', 'DAGS_FOLDER/ergast_kafka_fetch_and_publish.py', '--cfg-path', '/tmp/tmp3ts1ivwq']
[2024-06-19T14:26:23.286+0000] {standard_task_runner.py:91} INFO - Job 42: Subtask fetch_and_publish_constructor
[2024-06-19T14:26:23.320+0000] {task_command.py:426} INFO - Running <TaskInstance: f1_data_pipeline_kafka_fetchAndPublish.fetch_and_publish_constructor scheduled__2024-06-18T00:00:00+00:00 [running]> on host 7f714baf6013
[2024-06-19T14:26:23.375+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='f1_data_pipeline_kafka_fetchAndPublish' AIRFLOW_CTX_TASK_ID='fetch_and_publish_constructor' AIRFLOW_CTX_EXECUTION_DATE='2024-06-18T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-06-18T00:00:00+00:00'
[2024-06-19T14:26:23.376+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-06-19T14:26:23.465+0000] {logging_mixin.py:188} INFO - tried to find producer
[2024-06-19T14:26:23.467+0000] {conn.py:362} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('192.168.32.6', 9092)]>: connecting to kafka:9092 [('192.168.32.6', 9092) IPv4]
[2024-06-19T14:26:23.467+0000] {conn.py:957} INFO - Probing node bootstrap-0 broker version
[2024-06-19T14:26:23.468+0000] {conn.py:393} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connecting> [IPv4 ('192.168.32.6', 9092)]>: Connection complete.
[2024-06-19T14:26:23.570+0000] {conn.py:1019} INFO - Broker version identified as 2.6.0
[2024-06-19T14:26:23.570+0000] {conn.py:1020} INFO - Set configuration api_version=(2, 6, 0) to skip auto check_version requests on startup
[2024-06-19T14:26:23.724+0000] {conn.py:362} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('192.168.32.6', 9092)]>: connecting to kafka:9092 [('192.168.32.6', 9092) IPv4]
[2024-06-19T14:26:23.724+0000] {conn.py:393} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connecting> [IPv4 ('192.168.32.6', 9092)]>: Connection complete.
[2024-06-19T14:26:23.724+0000] {conn.py:673} INFO - <BrokerConnection node_id=bootstrap-0 host=kafka:9092 <connected> [IPv4 ('192.168.32.6', 9092)]>: Closing connection. 
[2024-06-19T14:26:23.835+0000] {kafka.py:486} INFO - Closing the Kafka producer with 9223372036.0 secs timeout.
[2024-06-19T14:26:23.839+0000] {conn.py:673} INFO - <BrokerConnection node_id=1 host=kafka:9092 <connected> [IPv4 ('192.168.32.6', 9092)]>: Closing connection. 
[2024-06-19T14:26:23.839+0000] {logging_mixin.py:188} INFO - Total number of constructors fetched: 10
[2024-06-19T14:26:23.840+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-06-19T14:26:23.840+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-06-19T14:26:23.846+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=f1_data_pipeline_kafka_fetchAndPublish, task_id=fetch_and_publish_constructor, run_id=scheduled__2024-06-18T00:00:00+00:00, execution_date=20240618T000000, start_date=20240619T142623, end_date=20240619T142623
[2024-06-19T14:26:23.859+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-06-19T14:26:23.870+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-06-19T14:26:23.871+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
